{
    "abstract": "Native listeners neurally integrate iconic gestures with speech, which can enhance degraded speech comprehension. However, it is unknown how non-native listeners neurally integrate speech and gestures, as they might process visual semantic context differently than natives. We recorded EEG while native and highly-proficient non-native listeners watched videos of an actress uttering an action verb in clear or degraded speech, accompanied by a matching ('to drive'+driving gesture) or mismatching gesture ('to drink'+mixing gesture). Degraded speech elicited an enhanced N400 amplitude compared to clear speech in both groups, revealing an increase in neural resources needed to resolve the spoken input. A larger N400 effect was found in clear speech for non-natives compared to natives, but in degraded speech only for natives. Non-native listeners might thus process gesture more strongly than natives when speech is clear, but need more auditory cues to facilitate access to gestural semantic information when speech is degraded.",
    "author_highlights": [
        {
            "endOffset": 9830,
            "sentence": "Natives and non-natives neurally integrate speech and gestures differently.",
            "startOffset": 9755
        },
        {
            "endOffset": 9897,
            "sentence": "Native listeners show an N400 effect in clear and degraded speech.",
            "startOffset": 9831
        },
        {
            "endOffset": 9972,
            "sentence": "Non-native listeners show an N400 effect in clear but not degraded speech.",
            "startOffset": 9898
        },
        {
            "endOffset": 10045,
            "sentence": "Non-natives process gestures stronger than natives when speech is clear.",
            "startOffset": 9973
        },
        {
            "endOffset": 10126,
            "sentence": "Non-natives require more auditory cues to utilize gestural semantic information.",
            "startOffset": 10046
        }
    ],
    "bib_entries": {
        "b0005": {
            "authors": [
                {
                    "first": "Jennifer",
                    "initial": "J.",
                    "last": "Aydelott"
                },
                {
                    "first": "Frederic",
                    "initial": "F.",
                    "last": "Dick"
                },
                {
                    "first": "Debra L.",
                    "initial": "D.L.",
                    "last": "Mills"
                }
            ],
            "doi": "10.1111/j.1469-8986.2006.00448.x",
            "firstpage": "454",
            "issn": "00485772",
            "lastpage": "464",
            "pmid": "16965607",
            "pub_year": 2006,
            "title": "Effects of acoustic distortion and semantic context on event-related potentials to spoken words",
            "volume": "43"
        },
        "b0010": {
            "authors": [
                {
                    "first": "Giosu\u00e8",
                    "initial": "G.",
                    "last": "Baggio"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Hagoort"
                }
            ],
            "doi": "10.1080/01690965.2010.542671",
            "firstpage": "1338",
            "issn": "01690965",
            "lastpage": "1367",
            "pub_year": 2011,
            "title": "The balance between memory and unification in semantics: A dynamic account of the N400",
            "volume": "26"
        },
        "b0015": null,
        "b0020": {
            "authors": [
                {
                    "first": "Geoffrey",
                    "initial": "G.",
                    "last": "Beattie"
                },
                {
                    "first": "Heather",
                    "initial": "H.",
                    "last": "Shovelton"
                }
            ],
            "doi": "10.1177/0261927X99018004005",
            "firstpage": "438",
            "issn": "0261927X",
            "lastpage": "462",
            "pub_year": 1999,
            "title": "Mapping the range of information contained in the iconic hand gestures that accompany spontaneous speech",
            "volume": "18"
        },
        "b0025": {
            "authors": [
                {
                    "first": "Geoffrey",
                    "initial": "G.",
                    "last": "Beattie"
                },
                {
                    "first": "Heather",
                    "initial": "H.",
                    "last": "Shovelton"
                }
            ],
            "doi": "10.1348/000712602162526",
            "firstpage": "179",
            "issn": "20448295",
            "lastpage": "192",
            "pmid": "12031146",
            "pub_year": 2002,
            "title": "An experimental investigation of some properties of individual iconic gestures that mediate their communicative power",
            "volume": "93"
        },
        "b0030": {
            "authors": [
                {
                    "first": "Emmanuel",
                    "initial": "E.",
                    "last": "Biau"
                },
                {
                    "first": "Salvador",
                    "initial": "S.",
                    "last": "Soto-Faraco"
                }
            ],
            "doi": "10.1016/j.bandl.2012.10.008",
            "firstpage": "143",
            "issn": "0093934X",
            "lastpage": "152",
            "pmid": "23333667",
            "pub_year": 2013,
            "title": "Beat gestures modulate auditory integration in speech perception",
            "volume": "124"
        },
        "b0035": {
            "authors": [
                {
                    "first": "Emmanuel",
                    "initial": "E.",
                    "last": "Biau"
                },
                {
                    "first": "Salvador",
                    "initial": "S.",
                    "last": "Soto-Faraco"
                }
            ],
            "doi": "10.3389/fnhum.2015.00527",
            "issn": "16625161",
            "pub_year": 2015,
            "title": "Synchronization by the hand: The sight of gestures modulates low-frequency activity in brain responses to continuous speech",
            "volume": "9"
        },
        "b0040": {
            "authors": [
                {
                    "first": "Emmanuel",
                    "initial": "E.",
                    "last": "Biau"
                },
                {
                    "first": "Mireia",
                    "initial": "M.",
                    "last": "Torralba"
                },
                {
                    "first": "Lluis",
                    "initial": "L.",
                    "last": "Fuentemilla"
                },
                {
                    "first": "Ruth",
                    "initial": "R.",
                    "last": "de Diego Balaguer"
                },
                {
                    "first": "Salvador",
                    "initial": "S.",
                    "last": "Soto-Faraco"
                }
            ],
            "doi": "10.1016/j.cortex.2014.11.018",
            "firstpage": "76",
            "issn": "00109452",
            "lastpage": "85",
            "pmid": "25595613",
            "pub_year": 2015,
            "title": "Speaker's hand gestures modulate speech perception through phase resetting of ongoing neural oscillations",
            "volume": "68"
        },
        "b0045": null,
        "b0050": {
            "authors": [
                {
                    "first": "V\u00e9ronique",
                    "initial": "V.",
                    "last": "Boulenger"
                },
                {
                    "first": "Michel",
                    "initial": "M.",
                    "last": "Hoen"
                },
                {
                    "first": "Caroline",
                    "initial": "C.",
                    "last": "Jacquier"
                },
                {
                    "first": "Fanny",
                    "initial": "F.",
                    "last": "Meunier"
                }
            ],
            "doi": "10.1016/j.bandl.2010.09.011",
            "firstpage": "51",
            "issn": "0093934X",
            "lastpage": "63",
            "pub_year": 2011,
            "title": "Interplay between acoustic/phonetic and semantic processes during spoken sentence comprehension: An ERP study",
            "volume": "116"
        },
        "b0055": {
            "authors": [
                {
                    "first": "Ann R.",
                    "initial": "A.R.",
                    "last": "Bradlow"
                },
                {
                    "first": "Jennifer A.",
                    "initial": "J.A.",
                    "last": "Alexander"
                }
            ],
            "doi": "10.1121/1.2642103",
            "firstpage": "2339",
            "issn": "00014966",
            "lastpage": "2349",
            "pmid": "17471746",
            "pub_year": 2007,
            "title": "Semantic and phonetic enhancements for speech-in-noise recognition by native and non-native listeners",
            "volume": "121"
        },
        "b0060": {
            "authors": [
                {
                    "first": "Ann R.",
                    "initial": "A.R.",
                    "last": "Bradlow"
                },
                {
                    "first": "Tessa",
                    "initial": "T.",
                    "last": "Bent"
                }
            ],
            "doi": "10.1121/1.1487837",
            "firstpage": "272",
            "issn": "00014966",
            "lastpage": "284",
            "pmid": "12141353",
            "pub_year": 2002,
            "title": "The clear speech effect for non-native listeners",
            "volume": "112"
        },
        "b0070": {
            "authors": [
                {
                    "first": "J. F.",
                    "initial": "J.F.",
                    "last": "Connolly"
                },
                {
                    "first": "N. A.",
                    "initial": "N.A.",
                    "last": "Phillips"
                },
                {
                    "first": "S. H.",
                    "initial": "S.H.",
                    "last": "Stewart"
                },
                {
                    "first": "W. G.",
                    "initial": "W.G.",
                    "last": "Brake"
                }
            ],
            "doi": "10.1016/0093-934X(92)90018-A",
            "firstpage": "1",
            "issn": "0093934X",
            "lastpage": "18",
            "pmid": "1643505",
            "pub_year": 1992,
            "title": "Event-related potential sensitivity to acoustic and semantic properties of terminal words in sentences",
            "volume": "43"
        },
        "b0075": {
            "authors": [
                {
                    "first": "Carlos",
                    "initial": "C.",
                    "last": "Cornejo"
                },
                {
                    "first": "Franco",
                    "initial": "F.",
                    "last": "Simonetti"
                },
                {
                    "first": "Agust\u00edn",
                    "initial": "A.",
                    "last": "Ib\u00e1\u00f1ez"
                },
                {
                    "first": "Nerea",
                    "initial": "N.",
                    "last": "Aldunate"
                },
                {
                    "first": "Francisco",
                    "initial": "F.",
                    "last": "Ceric"
                },
                {
                    "first": "Vladimir",
                    "initial": "V.",
                    "last": "L\u00f3pez"
                },
                {
                    "first": "Rafael E.",
                    "initial": "R.E.",
                    "last": "N\u00fa\u00f1ez"
                }
            ],
            "doi": "10.1016/j.bandc.2008.12.005",
            "firstpage": "42",
            "issn": "02782626",
            "lastpage": "52",
            "pmid": "19200632",
            "pub_year": 2009,
            "title": "Gesture and metaphor comprehension: Electrophysiological evidence of cross-modal coordination by audiovisual stimulation",
            "volume": "70"
        },
        "b0080": {
            "authors": [
                {
                    "first": "Tove Irene",
                    "initial": "T.I.",
                    "last": "Dahl"
                },
                {
                    "first": "Susanne",
                    "initial": "S.",
                    "last": "Ludvigsen"
                }
            ],
            "doi": "10.1111/modl.12124",
            "firstpage": "813",
            "issn": "00267902",
            "lastpage": "833",
            "pub_year": 2014,
            "title": "How i see what you're saying: The role of gestures in native and foreign language listening comprehension",
            "volume": "98"
        },
        "b0085": {
            "authors": [
                {
                    "first": "Anthony Steven",
                    "initial": "A.S.",
                    "last": "Dick"
                },
                {
                    "first": "Eva H.",
                    "initial": "E.H.",
                    "last": "Mok"
                },
                {
                    "first": "Anjali Raja",
                    "initial": "A.R.",
                    "last": "Beharelle"
                },
                {
                    "first": "Susan",
                    "initial": "S.",
                    "last": "Goldin-Meadow"
                },
                {
                    "first": "Steven L.",
                    "initial": "S.L.",
                    "last": "Small"
                }
            ],
            "doi": "10.1002/hbm.22222",
            "firstpage": "900",
            "issn": "10659471",
            "lastpage": "917",
            "pmid": "23238964",
            "pub_year": 2014,
            "title": "Frontal and temporal contributions to understanding the iconic co-speech gestures that accompany speech",
            "volume": "35"
        },
        "b0090": {
            "authors": [
                {
                    "first": "Diana",
                    "initial": "D.",
                    "last": "Dimitrova"
                },
                {
                    "first": "Mingyuan",
                    "initial": "M.",
                    "last": "Chu"
                },
                {
                    "first": "Lin",
                    "initial": "L.",
                    "last": "Wang"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Hagoort"
                }
            ],
            "doi": "10.1162/jocn_a_00963",
            "firstpage": "1255",
            "issn": "0898929X",
            "lastpage": "1269",
            "pmid": "27027421",
            "pub_year": 2016,
            "title": "Beat that word: How listeners integrate beat gesture and focus in multimodal speech discourse",
            "volume": "28"
        },
        "b0095": {
            "authors": [
                {
                    "first": "Linda",
                    "initial": "L.",
                    "last": "Drijvers"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                }
            ],
            "doi": "10.1044/2016_JSLHR-H-16-0101",
            "firstpage": "212",
            "issn": "10924388",
            "lastpage": "222",
            "pmid": "27960196",
            "pub_year": 2017,
            "title": "Visual context enhanced: The joint contribution of iconic gestures and visible speech to degraded speech comprehension",
            "volume": "60"
        },
        "b0100": {
            "authors": [
                {
                    "first": "Mirjam",
                    "initial": "M.",
                    "last": "Ernestus"
                },
                {
                    "first": "Mirte E.",
                    "initial": "M.E.",
                    "last": "Dikmans"
                },
                {
                    "first": "Ghislaine",
                    "initial": "G.",
                    "last": "Giezenaar"
                }
            ],
            "doi": "10.1075/dujal.6.1.01ern",
            "firstpage": "1",
            "issn": "22117245",
            "lastpage": "20",
            "pub_year": 2017,
            "title": "Advanced second language learners experience difficulties processing reduced word pronunciation variants",
            "volume": "6"
        },
        "b0105": {
            "authors": [
                {
                    "first": "Isabelle B.",
                    "initial": "I.B.",
                    "last": "Gat"
                },
                {
                    "first": "Robert W.",
                    "initial": "R.W.",
                    "last": "Keith"
                }
            ],
            "doi": "10.3109/00206097809101303",
            "firstpage": "339",
            "issn": "14992027",
            "lastpage": "345",
            "pmid": "687239",
            "pub_year": 1978,
            "title": "An effect of linguistic experience: Auditory word discrimination by native and non-native speakers of english",
            "volume": "17"
        },
        "b0110": null,
        "b0115": {
            "authors": [
                {
                    "first": "Narly",
                    "initial": "N.",
                    "last": "Golestani"
                },
                {
                    "first": "Stuart",
                    "initial": "S.",
                    "last": "Rosen"
                },
                {
                    "first": "Sophie K.",
                    "initial": "S.K.",
                    "last": "Scott"
                }
            ],
            "doi": "10.1017/S1366728909990150",
            "firstpage": "385",
            "issn": "13667289",
            "lastpage": "392",
            "pub_year": 2009,
            "title": "Native-language benefit for understanding speech-in-noise: The contribution of semantics",
            "volume": "12"
        },
        "b0120": {
            "authors": [
                {
                    "first": "Antonia",
                    "initial": "A.",
                    "last": "Green"
                },
                {
                    "first": "Benjamin",
                    "initial": "B.",
                    "last": "Straube"
                },
                {
                    "first": "Susanne",
                    "initial": "S.",
                    "last": "Weis"
                },
                {
                    "first": "Andreas",
                    "initial": "A.",
                    "last": "Jansen"
                },
                {
                    "first": "Klaus",
                    "initial": "K.",
                    "last": "Willmes"
                },
                {
                    "first": "Kerstin",
                    "initial": "K.",
                    "last": "Konrad"
                },
                {
                    "first": "Tilo",
                    "initial": "T.",
                    "last": "Kircher"
                }
            ],
            "doi": "10.1002/hbm.20753",
            "firstpage": "3309",
            "issn": "10659471",
            "lastpage": "3324",
            "pmid": "19350562",
            "pub_year": 2009,
            "title": "Neural integration of iconic and unrelated coverbal gestures: A functional MRI study",
            "volume": "30"
        },
        "b0125": {
            "authors": [
                {
                    "first": "Boukje",
                    "initial": "B.",
                    "last": "Habets"
                },
                {
                    "first": "Sotaro",
                    "initial": "S.",
                    "last": "Kita"
                },
                {
                    "first": "Zeshu",
                    "initial": "Z.",
                    "last": "Shao"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zyurek"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Hagoort"
                }
            ],
            "doi": "10.1162/jocn.2010.21462",
            "firstpage": "1845",
            "issn": "0898929X",
            "lastpage": "1854",
            "pmid": "20201632",
            "pub_year": 2011,
            "title": "The role of synchrony and ambiguity in speech-gesture integration during comprehension",
            "volume": "23"
        },
        "b0130": {
            "authors": [
                {
                    "first": "Valerie",
                    "initial": "V.",
                    "last": "Hazan"
                },
                {
                    "first": "Anke",
                    "initial": "A.",
                    "last": "Sennema"
                },
                {
                    "first": "Andrew",
                    "initial": "A.",
                    "last": "Faulkner"
                },
                {
                    "first": "Marta",
                    "initial": "M.",
                    "last": "Ortega-Llebaria"
                },
                {
                    "first": "Midori",
                    "initial": "M.",
                    "last": "Iba"
                },
                {
                    "first": "Hyunsong",
                    "initial": "H.",
                    "last": "Chung"
                }
            ],
            "doi": "10.1121/1.2166611",
            "firstpage": "1740",
            "issn": "00014966",
            "lastpage": "1751",
            "pmid": "16583916",
            "pub_year": 2006,
            "title": "The use of visual cues in the perception of non-native consonant contrasts",
            "volume": "119"
        },
        "b0135": {
            "authors": [
                {
                    "first": "Yifei",
                    "initial": "Y.",
                    "last": "He"
                },
                {
                    "first": "Helge",
                    "initial": "H.",
                    "last": "Gebhardt"
                },
                {
                    "first": "Miriam",
                    "initial": "M.",
                    "last": "Steines"
                },
                {
                    "first": "Gebhard",
                    "initial": "G.",
                    "last": "Sammer"
                },
                {
                    "first": "Tilo",
                    "initial": "T.",
                    "last": "Kircher"
                },
                {
                    "first": "Arne",
                    "initial": "A.",
                    "last": "Nagels"
                },
                {
                    "first": "Benjamin",
                    "initial": "B.",
                    "last": "Straube"
                }
            ],
            "doi": "10.1016/j.neuropsychologia.2015.04.018",
            "firstpage": "27",
            "issn": "00283932",
            "lastpage": "42",
            "pmid": "25900470",
            "pub_year": 2015,
            "title": "The EEG and fMRI signatures of neural integration: An investigation of meaningful gestures and corresponding speech",
            "volume": "72"
        },
        "b0140": {
            "authors": [
                {
                    "first": "Henning",
                    "initial": "H.",
                    "last": "Holle"
                },
                {
                    "first": "Thomas C.",
                    "initial": "T.C.",
                    "last": "Gunter"
                }
            ],
            "doi": "10.1162/jocn.2007.19.7.1175",
            "firstpage": "1175",
            "issn": "0898929X",
            "lastpage": "1192",
            "pmid": "17583993",
            "pub_year": 2007,
            "title": "The role of iconic gestures in speech disambiguation: ERP evidence",
            "volume": "19"
        },
        "b0145": {
            "authors": [
                {
                    "first": "Henning",
                    "initial": "H.",
                    "last": "Holle"
                },
                {
                    "first": "Thomas C.",
                    "initial": "T.C.",
                    "last": "Gunter"
                },
                {
                    "first": "Shirley Ann",
                    "initial": "S.A.",
                    "last": "R\u00fcschemeyer"
                },
                {
                    "first": "Andreas",
                    "initial": "A.",
                    "last": "Hennenlotter"
                },
                {
                    "first": "Marco",
                    "initial": "M.",
                    "last": "Iacoboni"
                }
            ],
            "doi": "10.1016/j.neuroimage.2007.10.055",
            "firstpage": "2010",
            "issn": "10538119",
            "lastpage": "2024",
            "pmid": "18093845",
            "pub_year": 2008,
            "title": "Neural correlates of the processing of co-speech gestures",
            "volume": "39"
        },
        "b0150": {
            "authors": [
                {
                    "first": "Henning",
                    "initial": "H.",
                    "last": "Holle"
                },
                {
                    "first": "Christian",
                    "initial": "C.",
                    "last": "Obermeier"
                },
                {
                    "first": "Maren",
                    "initial": "M.",
                    "last": "Schmidt-Kassow"
                },
                {
                    "first": "Angela D.",
                    "initial": "A.D.",
                    "last": "Friederici"
                },
                {
                    "first": "Jamie",
                    "initial": "J.",
                    "last": "Ward"
                },
                {
                    "first": "Thomas C.",
                    "initial": "T.C.",
                    "last": "Gunter"
                }
            ],
            "doi": "10.3389/fpsyg.2012.00074",
            "issn": "16641078",
            "pub_year": 2012,
            "title": "Gesture facilitates the syntactic analysis of speech",
            "volume": "3"
        },
        "b0155": {
            "authors": [
                {
                    "first": "Henning",
                    "initial": "H.",
                    "last": "Holle"
                },
                {
                    "first": "Jonas",
                    "initial": "J.",
                    "last": "Obleser"
                },
                {
                    "first": "Shirley Ann",
                    "initial": "S.A.",
                    "last": "Rueschemeyer"
                },
                {
                    "first": "Thomas C.",
                    "initial": "T.C.",
                    "last": "Gunter"
                }
            ],
            "doi": "10.1016/j.neuroimage.2009.08.058",
            "firstpage": "875",
            "issn": "10538119",
            "lastpage": "884",
            "pmid": "19733670",
            "pub_year": 2010,
            "title": "Integration of iconic gestures and speech in left superior temporal areas boosts speech comprehension under adverse listening conditions",
            "volume": "49"
        },
        "b0160": null,
        "b0165": {
            "authors": [
                {
                    "first": "Judith",
                    "initial": "J.",
                    "last": "Holler"
                },
                {
                    "first": "Idil",
                    "initial": "I.",
                    "last": "Kokal"
                },
                {
                    "first": "Ivan",
                    "initial": "I.",
                    "last": "Toni"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Hagoort"
                },
                {
                    "first": "Spencer D.",
                    "initial": "S.D.",
                    "last": "Kelly"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                }
            ],
            "doi": "10.1093/scan/nsu047",
            "firstpage": "255",
            "issn": "17495016",
            "lastpage": "261",
            "pmid": "24652857",
            "pub_year": 2015,
            "title": "Eye'm talking to you: Speakers' gaze direction modulates co-speech gesture processing in the right MTG",
            "volume": "10"
        },
        "b0175": {
            "authors": [
                {
                    "first": "Judith",
                    "initial": "J.",
                    "last": "Holler"
                },
                {
                    "first": "Heather",
                    "initial": "H.",
                    "last": "Shovelton"
                },
                {
                    "first": "Geoffrey",
                    "initial": "G.",
                    "last": "Beattie"
                }
            ],
            "doi": "10.1007/s10919-008-0063-9",
            "firstpage": "73",
            "issn": "01915886",
            "lastpage": "88",
            "pub_year": 2009,
            "title": "Do Iconic Hand Gestures Really Contribute to the Communication of Semantic Information in a Face-to-Face Context?",
            "volume": "33"
        },
        "b0185": {
            "authors": [
                {
                    "first": "Spencer D.",
                    "initial": "S.D.",
                    "last": "Kelly"
                },
                {
                    "first": "Dale J.",
                    "initial": "D.J.",
                    "last": "Barr"
                },
                {
                    "first": "R. Breckinridge",
                    "initial": "R.B.",
                    "last": "Church"
                },
                {
                    "first": "Katheryn",
                    "initial": "K.",
                    "last": "Lynch"
                }
            ],
            "doi": "10.1006/jmla.1999.2634",
            "firstpage": "577",
            "issn": "0749596X",
            "lastpage": "592",
            "pub_year": 1999,
            "title": "Offering a Hand to Pragmatic Understanding: The Role of Speech and Gesture in Comprehension and Memory",
            "volume": "40"
        },
        "b0190": {
            "authors": [
                {
                    "first": "Spencer D.",
                    "initial": "S.D.",
                    "last": "Kelly"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Creigh"
                },
                {
                    "first": "James",
                    "initial": "J.",
                    "last": "Bartolotti"
                }
            ],
            "doi": "10.1162/jocn.2009.21254",
            "firstpage": "683",
            "issn": "0898929X",
            "lastpage": "694",
            "pmid": "19413483",
            "pub_year": 2010,
            "title": "Integrating speech and iconic gestures in a stroop-like task: Evidence for automatic processing",
            "volume": "22"
        },
        "b0195": {
            "authors": [
                {
                    "first": "Spencer",
                    "initial": "S.",
                    "last": "Kelly"
                },
                {
                    "first": "Meghan",
                    "initial": "M.",
                    "last": "Healey"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                },
                {
                    "first": "Judith",
                    "initial": "J.",
                    "last": "Holler"
                }
            ],
            "doi": "10.3758/s13423-014-0681-7",
            "firstpage": "517",
            "issn": "10699384",
            "lastpage": "523",
            "pmid": "25002252",
            "pub_year": 2015,
            "title": "The processing of speech, gesture, and action during language comprehension",
            "volume": "22"
        },
        "b0200": {
            "authors": [
                {
                    "first": "Spencer D.",
                    "initial": "S.D.",
                    "last": "Kelly"
                },
                {
                    "first": "Corinne",
                    "initial": "C.",
                    "last": "Kravitz"
                },
                {
                    "first": "Michael",
                    "initial": "M.",
                    "last": "Hopkins"
                }
            ],
            "doi": "10.1016/S0093-934X(03)00335-3",
            "firstpage": "253",
            "issn": "0093934X",
            "lastpage": "260",
            "pmid": "15010257",
            "pub_year": 2004,
            "title": "Neural correlates of bimodal speech and gesture comprehension",
            "volume": "89"
        },
        "b0205": {
            "authors": [
                {
                    "first": "Spencer D.",
                    "initial": "S.D.",
                    "last": "Kelly"
                },
                {
                    "first": "Angela L.",
                    "initial": "A.L.",
                    "last": "Lee"
                }
            ],
            "doi": "10.1080/01690965.2011.581125",
            "firstpage": "793",
            "issn": "01690965",
            "lastpage": "807",
            "pub_year": 2012,
            "title": "When actions speak too much louder than words: Hand gestures disrupt word learning when phonetic demands are high",
            "volume": "27"
        },
        "b0210": {
            "authors": [
                {
                    "first": "Spencer D.",
                    "initial": "S.D.",
                    "last": "Kelly"
                },
                {
                    "first": "Sarah",
                    "initial": "S.",
                    "last": "Ward"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Creigh"
                },
                {
                    "first": "James",
                    "initial": "J.",
                    "last": "Bartolotti"
                }
            ],
            "doi": "10.1016/j.bandl.2006.07.008",
            "firstpage": "222",
            "issn": "0093934X",
            "lastpage": "233",
            "pmid": "16997367",
            "pub_year": 2007,
            "title": "An intentional stance modulates the integration of gesture and speech during comprehension",
            "volume": "101"
        },
        "b0215": {
            "authors": [
                {
                    "first": "Markus",
                    "initial": "M.",
                    "last": "Kiefer"
                },
                {
                    "first": "Matthias",
                    "initial": "M.",
                    "last": "Weisbrod"
                },
                {
                    "first": "Isabel",
                    "initial": "I.",
                    "last": "Kern"
                },
                {
                    "first": "Sabine",
                    "initial": "S.",
                    "last": "Maier"
                },
                {
                    "first": "Manfred",
                    "initial": "M.",
                    "last": "Spitzer"
                }
            ],
            "doi": "10.1006/brln.1998.1979",
            "firstpage": "377",
            "issn": "0093934X",
            "lastpage": "408",
            "pmid": "9743549",
            "pub_year": 1998,
            "title": "Right hemisphere activation during indirect semantic priming: Evidence from event-related potentials",
            "volume": "64"
        },
        "b0220": {
            "authors": [
                {
                    "first": "Robert M.",
                    "initial": "R.M.",
                    "last": "Krauss"
                },
                {
                    "first": "Palmer",
                    "initial": "P.",
                    "last": "Morrel-Samuels"
                },
                {
                    "first": "Christina",
                    "initial": "C.",
                    "last": "Colasante"
                }
            ],
            "doi": "10.1037/0022-3514.61.5.743",
            "firstpage": "743",
            "issn": "00223514",
            "lastpage": "754",
            "pmid": "1753329",
            "pub_year": 1991,
            "title": "Do Conversational Hand Gestures Communicate?",
            "volume": "61"
        },
        "b0225": {
            "authors": [
                {
                    "first": "Marta",
                    "initial": "M.",
                    "last": "Kutas"
                },
                {
                    "first": "Kara D.",
                    "initial": "K.D.",
                    "last": "Federmeier"
                }
            ],
            "doi": "10.1016/S1364-6613(00)01560-6",
            "firstpage": "463",
            "issn": "13646613",
            "lastpage": "470",
            "pub_year": 2000,
            "title": "Electrophysiology reveals semantic memory use in language comprehension",
            "volume": "4"
        },
        "b0230": {
            "authors": [
                {
                    "first": "Marta",
                    "initial": "M.",
                    "last": "Kutas"
                },
                {
                    "first": "Kara D.",
                    "initial": "K.D.",
                    "last": "Federmeier"
                }
            ],
            "doi": "10.1146/annurev.psych.093008.131123",
            "firstpage": "621",
            "issn": "00664308",
            "lastpage": "647",
            "pmid": "20809790",
            "pub_year": 2011,
            "title": "Thirty years and counting: Finding meaning in the N400 component of the event-related brain potential (ERP)",
            "volume": "62"
        },
        "b0235": {
            "authors": [
                {
                    "first": "Kristin",
                    "initial": "K.",
                    "last": "Lemh\u00f6fer"
                },
                {
                    "first": "Mirjam",
                    "initial": "M.",
                    "last": "Broersma"
                }
            ],
            "doi": "10.3758/s13428-011-0146-0",
            "firstpage": "325",
            "issn": "1554351X",
            "lastpage": "343",
            "pmid": "21898159",
            "pub_year": 2012,
            "title": "Introducing LexTALE: A quick and valid Lexical Test for Advanced Learners of English",
            "volume": "44"
        },
        "b0240": null,
        "b0245": {
            "authors": [
                {
                    "first": "Eric",
                    "initial": "E.",
                    "last": "Maris"
                },
                {
                    "first": "Robert",
                    "initial": "R.",
                    "last": "Oostenveld"
                }
            ],
            "doi": "10.1016/j.jneumeth.2007.03.024",
            "firstpage": "177",
            "issn": "01650270",
            "lastpage": "190",
            "pmid": "17517438",
            "pub_year": 2007,
            "title": "Nonparametric statistical testing of EEG- and MEG-data",
            "volume": "164"
        },
        "b0250": {
            "authors": [
                {
                    "first": "Lynn Hansberry",
                    "initial": "L.H.",
                    "last": "Mayo"
                },
                {
                    "first": "Mary",
                    "initial": "M.",
                    "last": "Florentine"
                },
                {
                    "first": "S\u00f8ren",
                    "initial": "S.",
                    "last": "Buus"
                }
            ],
            "doi": "10.1044/jslhr.4003.686",
            "firstpage": "686",
            "issn": "10924388",
            "lastpage": "693",
            "pmid": "9210123",
            "pub_year": 1997,
            "title": "Age of second-language acquisition and perception of speech in noise",
            "volume": "40"
        },
        "b0255": null,
        "b0265": {
            "authors": [
                {
                    "first": "Christian",
                    "initial": "C.",
                    "last": "Obermeier"
                },
                {
                    "first": "Thomas",
                    "initial": "T.",
                    "last": "Dolk"
                },
                {
                    "first": "Thomas C.",
                    "initial": "T.C.",
                    "last": "Gunter"
                }
            ],
            "doi": "10.1016/j.cortex.2011.02.007",
            "firstpage": "857",
            "issn": "00109452",
            "lastpage": "870",
            "pmid": "21397223",
            "pub_year": 2012,
            "title": "The benefit of gestures during communication: Evidence from hearing and hearing-impaired individuals",
            "volume": "48"
        },
        "b0270": {
            "authors": [
                {
                    "first": "Christian",
                    "initial": "C.",
                    "last": "Obermeier"
                },
                {
                    "first": "Henning",
                    "initial": "H.",
                    "last": "Holle"
                },
                {
                    "first": "Thomas C.",
                    "initial": "T.C.",
                    "last": "Gunter"
                }
            ],
            "doi": "10.1162/jocn.2010.21498",
            "firstpage": "1648",
            "issn": "0898929X",
            "lastpage": "1663",
            "pmid": "20350188",
            "pub_year": 2011,
            "title": "What iconic gesture fragments reveal about gesture-speech integration: When synchrony is lost, memory can help",
            "volume": "23"
        },
        "b0275": {
            "authors": [
                {
                    "first": "Jonas",
                    "initial": "J.",
                    "last": "Obleser"
                },
                {
                    "first": "Sonja A.",
                    "initial": "S.A.",
                    "last": "Kotz"
                }
            ],
            "doi": "10.1016/j.neuroimage.2010.12.020",
            "firstpage": "713",
            "issn": "10538119",
            "lastpage": "723",
            "pmid": "21172443",
            "pub_year": 2011,
            "title": "Multiple brain signatures of integration in the comprehension of degraded speech",
            "volume": "55"
        },
        "b0280": {
            "authors": [
                {
                    "first": "Georgina",
                    "initial": "G.",
                    "last": "Oliver"
                },
                {
                    "first": "Marianne",
                    "initial": "M.",
                    "last": "Gullberg"
                },
                {
                    "first": "Frauke",
                    "initial": "F.",
                    "last": "Hellwig"
                },
                {
                    "first": "Holger",
                    "initial": "H.",
                    "last": "Mitterer"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Indefrey"
                }
            ],
            "doi": "10.1017/S1366728912000089",
            "firstpage": "841",
            "issn": "13667289",
            "lastpage": "857",
            "pub_year": 2012,
            "title": "Acquiring L2 sentence comprehension: A longitudinal study of word monitoring in noise",
            "volume": "15"
        },
        "b0285": {
            "authors": [
                {
                    "first": "Robert",
                    "initial": "R.",
                    "last": "Oostenveld"
                },
                {
                    "first": "Pascal",
                    "initial": "P.",
                    "last": "Fries"
                },
                {
                    "first": "Eric",
                    "initial": "E.",
                    "last": "Maris"
                },
                {
                    "first": "Jan Mathijs",
                    "initial": "J.M.",
                    "last": "Schoffelen"
                }
            ],
            "doi": "10.1155/2011/156869",
            "issn": "16875265",
            "pmid": "21253357",
            "pub_year": 2011,
            "title": "FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data",
            "volume": "2011"
        },
        "b0290": {
            "authors": [
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                }
            ],
            "doi": "10.1098/rstb.2013.0296",
            "issn": "09628436",
            "pmid": "25092664",
            "pub_year": 2014,
            "title": "Hearing and seeing meaning in speech and gesture: Insights from brain and behaviour",
            "volume": "369"
        },
        "b0295": {
            "authors": [
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                },
                {
                    "first": "Roel M.",
                    "initial": "R.M.",
                    "last": "Willems"
                },
                {
                    "first": "Sotaro",
                    "initial": "S.",
                    "last": "Kita"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Hagoort"
                }
            ],
            "doi": "10.1162/jocn.2007.19.4.605",
            "firstpage": "605",
            "issn": "0898929X",
            "lastpage": "616",
            "pmid": "17381252",
            "pub_year": 2007,
            "title": "On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials",
            "volume": "19"
        },
        "b0300": {
            "authors": [
                {
                    "first": "Robert V.",
                    "initial": "R.V.",
                    "last": "Shannon"
                },
                {
                    "first": "Fan Gang",
                    "initial": "F.G.",
                    "last": "Zeng"
                },
                {
                    "first": "Vivek",
                    "initial": "V.",
                    "last": "Kamath"
                },
                {
                    "first": "John",
                    "initial": "J.",
                    "last": "Wygonski"
                },
                {
                    "first": "Michael",
                    "initial": "M.",
                    "last": "Ekelid"
                }
            ],
            "doi": "10.1126/science.270.5234.303",
            "firstpage": "303",
            "issn": "00368075",
            "lastpage": "304",
            "pmid": "7569981",
            "pub_year": 1995,
            "title": "Speech recognition with primarily temporal cues",
            "volume": "270"
        },
        "b0305": {
            "authors": [
                {
                    "first": "Elizabeth A.",
                    "initial": "E.A.",
                    "last": "Sheehan"
                },
                {
                    "first": "Laura L.",
                    "initial": "L.L.",
                    "last": "Namy"
                },
                {
                    "first": "Debra L.",
                    "initial": "D.L.",
                    "last": "Mills"
                }
            ],
            "doi": "10.1016/j.bandl.2006.11.008",
            "firstpage": "246",
            "issn": "0093934X",
            "lastpage": "259",
            "pmid": "17250885",
            "pub_year": 2007,
            "title": "Developmental changes in neural activity to familiar words and gestures",
            "volume": "101"
        },
        "b0310": {
            "authors": [
                {
                    "first": "Takashi",
                    "initial": "T.",
                    "last": "Shimizu"
                },
                {
                    "first": "Kazumi",
                    "initial": "K.",
                    "last": "Makishima"
                },
                {
                    "first": "Masafumi",
                    "initial": "M.",
                    "last": "Yoshida"
                },
                {
                    "first": "Hidetoshi",
                    "initial": "H.",
                    "last": "Yamagishi"
                }
            ],
            "doi": "10.1016/S0385-8146(01)00133-X",
            "firstpage": "121",
            "issn": "03858146",
            "lastpage": "125",
            "pmid": "11893445",
            "pub_year": 2002,
            "title": "Effect of background noise on perception of English speech for Japanese listeners",
            "volume": "29"
        },
        "b0315": {
            "authors": [
                {
                    "first": "Jeremy I.",
                    "initial": "J.I.",
                    "last": "Skipper"
                },
                {
                    "first": "Susan",
                    "initial": "S.",
                    "last": "Goldin-Meadow"
                },
                {
                    "first": "Howard C.",
                    "initial": "H.C.",
                    "last": "Nusbaum"
                },
                {
                    "first": "Steven L.",
                    "initial": "S.L.",
                    "last": "Small"
                }
            ],
            "doi": "10.1016/j.cub.2009.02.051",
            "firstpage": "661",
            "issn": "09609822",
            "lastpage": "667",
            "pmid": "19327997",
            "pub_year": 2009,
            "title": "Gestures Orchestrate Brain Networks for Language Understanding",
            "volume": "19"
        },
        "b0320": {
            "authors": [
                {
                    "first": "Jeremy I.",
                    "initial": "J.I.",
                    "last": "Skipper"
                },
                {
                    "first": "Howard C.",
                    "initial": "H.C.",
                    "last": "Nusbaum"
                },
                {
                    "first": "Steven L.",
                    "initial": "S.L.",
                    "last": "Small"
                }
            ],
            "doi": "10.1017/CBO9780511541599.009",
            "firstpage": "250",
            "lastpage": "286",
            "pub_year": 2006,
            "title": "Lending a helping hand to hearing: Another motor theory of speech perception"
        },
        "b0325": {
            "authors": [
                {
                    "first": "Jeremy I.",
                    "initial": "J.I.",
                    "last": "Skipper"
                },
                {
                    "first": "Virginie",
                    "initial": "V.",
                    "last": "Van Wassenhove"
                },
                {
                    "first": "Howard C.",
                    "initial": "H.C.",
                    "last": "Nusbaum"
                },
                {
                    "first": "Steven L.",
                    "initial": "S.L.",
                    "last": "Small"
                }
            ],
            "doi": "10.1093/cercor/bhl147",
            "firstpage": "2387",
            "issn": "10473211",
            "lastpage": "2399",
            "pmid": "17218482",
            "pub_year": 2007,
            "title": "Hearing lips and seeing voices: How cortical areas supporting speech production mediate audiovisual speech perception",
            "volume": "17"
        },
        "b0330": {
            "authors": [
                {
                    "first": "Benjamin",
                    "initial": "B.",
                    "last": "Straube"
                },
                {
                    "first": "Antonia",
                    "initial": "A.",
                    "last": "Green"
                },
                {
                    "first": "Susanne",
                    "initial": "S.",
                    "last": "Weis"
                },
                {
                    "first": "Tilo",
                    "initial": "T.",
                    "last": "Kircher"
                }
            ],
            "doi": "10.1371/journal.pone.0051207",
            "issn": "19326203",
            "pmid": "23226488",
            "pub_year": 2012,
            "title": "A Supramodal Neural Network for Speech and Gesture Semantics: An fMRI Study",
            "volume": "7"
        },
        "b0335": {
            "authors": [
                {
                    "first": "Antje",
                    "initial": "A.",
                    "last": "Strau\u00df"
                },
                {
                    "first": "Sonja A.",
                    "initial": "S.A.",
                    "last": "Kotz"
                },
                {
                    "first": "Jonas",
                    "initial": "J.",
                    "last": "Obleser"
                }
            ],
            "doi": "10.1162/jocn_a_00389",
            "firstpage": "1383",
            "issn": "0898929X",
            "lastpage": "1395",
            "pmid": "23489145",
            "pub_year": 2013,
            "title": "Narrowed expectancies under degraded speech: Revisiting the N400",
            "volume": "25"
        },
        "b0340": {
            "authors": [
                {
                    "first": "Ayano",
                    "initial": "A.",
                    "last": "Sueyoshi"
                },
                {
                    "first": "Debra M.",
                    "initial": "D.M.",
                    "last": "Hardison"
                }
            ],
            "doi": "10.1111/j.0023-8333.2005.00320.x",
            "firstpage": "661",
            "issn": "00238333",
            "lastpage": "699",
            "pub_year": 2005,
            "title": "The role of gestures and facial cues in second language listening comprehension",
            "volume": "55"
        },
        "b0345": {
            "authors": [
                {
                    "first": "Lin",
                    "initial": "L.",
                    "last": "Wang"
                },
                {
                    "first": "Mingyuan",
                    "initial": "M.",
                    "last": "Chu"
                }
            ],
            "doi": "10.1016/j.neuropsychologia.2013.09.027",
            "firstpage": "2847",
            "issn": "00283932",
            "lastpage": "2855",
            "pmid": "24060845",
            "pub_year": 2013,
            "title": "The role of beat gesture and pitch accent in semantic processing: An ERP study",
            "volume": "51"
        },
        "b0350": {
            "authors": [
                {
                    "first": "Sander J.",
                    "initial": "S.J.",
                    "last": "Van Wijngaarden"
                },
                {
                    "first": "Herman J.M.",
                    "initial": "H.J.M.",
                    "last": "Steeneken"
                },
                {
                    "first": "Tammo",
                    "initial": "T.",
                    "last": "Houtgast"
                }
            ],
            "doi": "10.1121/1.1512289",
            "firstpage": "3004",
            "issn": "00014966",
            "lastpage": "3013",
            "pmid": "12509022",
            "pub_year": 2002,
            "title": "Quantifying the intelligibility of speech in noise for non-native talkers",
            "volume": "112"
        },
        "b0355": {
            "authors": [
                {
                    "first": "Roel M.",
                    "initial": "R.M.",
                    "last": "Willems"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Hagoort"
                }
            ],
            "doi": "10.1093/cercor/bhl141",
            "firstpage": "2322",
            "issn": "10473211",
            "lastpage": "2333",
            "pmid": "17159232",
            "pub_year": 2007,
            "title": "When language meets action: The neural integration of gesture and speech",
            "volume": "17"
        },
        "b0360": {
            "authors": [
                {
                    "first": "Roel M.",
                    "initial": "R.M.",
                    "last": "Willems"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                },
                {
                    "first": "Peter",
                    "initial": "P.",
                    "last": "Hagoort"
                }
            ],
            "doi": "10.1016/j.neuroimage.2009.05.066",
            "firstpage": "1992",
            "issn": "10538119",
            "lastpage": "2004",
            "pmid": "19497376",
            "pub_year": 2009,
            "title": "Differential roles for left inferior frontal and superior temporal cortex in multimodal integration of action and language",
            "volume": "47"
        },
        "b0365": {
            "authors": [
                {
                    "first": "Ying Choon",
                    "initial": "Y.C.",
                    "last": "Wu"
                },
                {
                    "first": "Seana",
                    "initial": "S.",
                    "last": "Coulson"
                }
            ],
            "doi": "10.1111/j.1469-8986.2005.00356.x",
            "firstpage": "654",
            "issn": "00485772",
            "lastpage": "667",
            "pmid": "16364061",
            "pub_year": 2005,
            "title": "Meaningful gestures: Electrophysiological indices of iconic gesture comprehension",
            "volume": "42"
        },
        "b0370": {
            "authors": [
                {
                    "first": "Ying Choon",
                    "initial": "Y.C.",
                    "last": "Wu"
                },
                {
                    "first": "Seana",
                    "initial": "S.",
                    "last": "Coulson"
                }
            ],
            "doi": "10.1016/j.bandl.2006.12.003",
            "firstpage": "234",
            "issn": "0093934X",
            "lastpage": "245",
            "pmid": "17222897",
            "pub_year": 2007,
            "title": "How iconic gestures enhance communication: An ERP study",
            "volume": "101"
        },
        "b0375": {
            "authors": [
                {
                    "first": "Ying Choon",
                    "initial": "Y.C.",
                    "last": "Wu"
                },
                {
                    "first": "Seana",
                    "initial": "S.",
                    "last": "Coulson"
                }
            ],
            "doi": "10.3758/BF03194028",
            "firstpage": "57",
            "issn": "10699384",
            "lastpage": "63",
            "pmid": "17546731",
            "pub_year": 2007,
            "title": "Iconic gestures prime related concepts: An ERP study",
            "volume": "14"
        },
        "b0380": {
            "authors": [
                {
                    "first": "Linjun",
                    "initial": "L.",
                    "last": "Zhang"
                },
                {
                    "first": "Yu",
                    "initial": "Y.",
                    "last": "Li"
                },
                {
                    "first": "Han",
                    "initial": "H.",
                    "last": "Wu"
                },
                {
                    "first": "Xin",
                    "initial": "X.",
                    "last": "Li"
                },
                {
                    "first": "Hua",
                    "initial": "H.",
                    "last": "Shu"
                },
                {
                    "first": "Yang",
                    "initial": "Y.",
                    "last": "Zhang"
                },
                {
                    "first": "Ping",
                    "initial": "P.",
                    "last": "Li"
                }
            ],
            "doi": "10.3389/fpsyg.2016.00908",
            "issn": "16641078",
            "pub_year": 2016,
            "title": "Effects of semantic context and fundamental frequency contours on mandarin speech recognition by second language learners",
            "volume": "7"
        },
        "b9015": {
            "authors": [
                {
                    "first": "Linda",
                    "initial": "L.",
                    "last": "Drijvers"
                },
                {
                    "first": "Asli",
                    "initial": "A.",
                    "last": "\u00d6zy\u00fcrek"
                },
                {
                    "first": "Ole",
                    "initial": "O.",
                    "last": "Jensen"
                }
            ],
            "doi": "10.1002/hbm.23987",
            "firstpage": "2075",
            "issn": "10659471",
            "lastpage": "2087",
            "pmid": "29380945",
            "pub_year": 2018,
            "title": "Hearing and seeing meaning in noise: Alpha, beta, and gamma oscillations predict gestural enhancement of degraded speech comprehension",
            "volume": "39"
        }
    },
    "body_text": [
        {
            "endOffset": 55996,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0070": {
                    "endOffset": 55994,
                    "startOffset": 55953
                }
            },
            "secId": "s0110",
            "sentence": "Since we did not find a difference in N400 peak latency but only in the onset of the N400 effect, we suggest this is due to the auditory cognitive load that degraded speech imposes on the listener (Connolly, Philips, Stewart, & Brake, 1992).",
            "startOffset": 55755,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 20526,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "It remains unknown whether the semantic information conveyed by gestures is used as much when both visible speech and gestures are available as visible cues to enhance speech comprehension.",
            "startOffset": 20337,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 22256,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "Unlike the sentential semantic context provided in the studies above, gestures might provide visual semantic context and semantic expectencies about a word when speech is degraded.",
            "startOffset": 22076,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 32596,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0235": {
                    "endOffset": 32594,
                    "startOffset": 32569
                }
            },
            "secId": "s0035",
            "sentence": "A score of 60% and higher is predicted to correlate with a B2 level or higher (Lemh\u00f6fer & Broersma, 2012).",
            "startOffset": 32490,
            "title": "LexTALE assessment"
        },
        {
            "endOffset": 21206,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0005": {
                    "endOffset": 21204,
                    "startOffset": 21078
                },
                "b0050": {
                    "endOffset": 21204,
                    "startOffset": 21078
                },
                "b0275": {
                    "endOffset": 21204,
                    "startOffset": 21078
                },
                "b0335": {
                    "endOffset": 21204,
                    "startOffset": 21078
                }
            },
            "secId": "s0010",
            "sentence": "These auditory electrophysiological studies have demonstrated that the N400 amplitude of a native listener is reduced in response to incongruent items that are acoustically degraded (e.g., a negative N400 amplitude when unifying an incongruent word with a preceding context in clear speech is less negative during degraded speech), or even absent when speech is too severely degraded (Aydelott, Dick, & Mills, 2006; Boulenger, Hoen, Jacquier, & Meunier, 2011; Obleser & Kotz, 2011; Strau\u00df, Kotz, & Obleser, 2013).",
            "startOffset": 20693,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 61981,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0010": {
                    "endOffset": 61979,
                    "startOffset": 61957
                }
            },
            "secId": "s0115",
            "sentence": "Previous literature has hypothesized that the N400 could reflect reverberating neural activity that is instantiated by a network consisting of memory/storage (MTG/STG), unification (LIFG) and control retrieval (dorsolateral prefrontal cortex) areas (Baggio & Hagoort, 2011).",
            "startOffset": 61707,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 30869,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0055": {
                    "endOffset": 30867,
                    "startOffset": 30822
                },
                "b0130": {
                    "endOffset": 30867,
                    "startOffset": 30822
                }
            },
            "secId": "s0020",
            "sentence": "This would fit with previous behavioral results that suggested that a certain signal clarity is required for non-natives for semantic information to be effective (e.g., Bradlow & Alexander, 2007; Hazan et al., 2006).",
            "startOffset": 30653,
            "title": "The present study"
        },
        {
            "endOffset": 37343,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "Participants sat in front of a computer monitor while holding a four-button box in an acoustically and electrically shielded room.",
            "startOffset": 37213,
            "title": "Procedure"
        },
        {
            "endOffset": 58354,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "We therefore suggest that this effect is even more enhanced for non-native listeners: when signal quality suffers and there are less auditory cues to map semantic information to, non-native listeners are less able than native listeners to benefit from semantic information from the gesture to boost comprehension and resolve the degraded auditory input.",
            "startOffset": 58001,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 18901,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "Their results showed that speech-gesture integration could enhance speech comprehension in noise (especially at a moderate noise level) and that this bimodal enhancement was reflected by an increased activation of left pSTS/STG.",
            "startOffset": 18673,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 47435,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "refoffsets": {
                "b0125": {
                    "endOffset": 47433,
                    "startOffset": 47388
                },
                "b0230": {
                    "endOffset": 47433,
                    "startOffset": 47388
                }
            },
            "secId": "s0085",
            "sentence": "For the analyses of our EEG data, we defined our time-window of interest (1.0\u20131.7, which corresponds to 300 ms after speech onset (at \u223c680 ms) until 1000 ms after speech onset, based on previous research on speech-gesture integration and N400 effects, and visual inspection of the waveforms (e.g., Habets et al., 2011; Kutas & Federmeier, 2014).",
            "startOffset": 47090,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 49903,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0095",
            "sentence": "Although the difference between native and non-native listeners in separate ERP waveforms per condition could not be compared, we did compare the N400 effects found in clear and degraded speech between the two groups (i.e., the interaction effect of nativeness and gesture congruence).",
            "startOffset": 49618,
            "title": "EEG data - native versus non-native listeners"
        },
        {
            "endOffset": 18566,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "They presented participants with videos that could either contain speech in a good signal-to-noise ratio, a moderate signal-to-noise ratio, or no speech.",
            "startOffset": 18413,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 28500,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "Furthermore, we expected a typical N400 effect when comparing a matching and mismatching gesture in clear speech, with a more negative N400 amplitude in response to mismatching gestures.",
            "startOffset": 28314,
            "title": "The present study"
        },
        {
            "endOffset": 17982,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "Contrary to the numerous studies on speech-gesture integration during clear speech processing, less is known about how native listeners integrate speech and gestures in adverse listening conditions.",
            "startOffset": 17784,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 19333,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "The sound in these videos was presented either clear, moderately degraded by noise-vocoding (6-band) or severely degraded by noise-vocoding (2-band).",
            "startOffset": 19184,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 43875,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "Lastly, there was a significant interaction between Noise-vocoding and Gesture (F(1, 22) = 43.87, p = <.001, Wilks' Lambda = 0.23, \u03b72 = 0.66), indicating that when the signal was clear and the gesture matched with the speech signal, native listeners answered more quickly.",
            "startOffset": 43603,
            "title": "Native listeners"
        },
        {
            "endOffset": 17784,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "Their results implied that speech and gesture are integrated most efficiently when they occur within a certain time span, because iconic gestures need speech to be disambiguated to fit within the speech context.",
            "startOffset": 17573,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 30652,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "This, in turn, might diminish how much non-natives can benefit from gestural information, and might result in no or a reduced N400 effect when comparing degraded speech and a mismatching gesture to degraded speech and a matching gesture.",
            "startOffset": 30415,
            "title": "The present study"
        },
        {
            "endOffset": 40133,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0245": {
                    "endOffset": 40046,
                    "startOffset": 40022
                }
            },
            "secId": "s0055",
            "sentence": "We used non-parametric cluster-based permutation tests (Maris & Oostenveld, 2007) to evaluate the differences between conditions within each listener group separately.",
            "startOffset": 39966,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 23228,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "Non-native listeners might utilize visual semantic cues that are conveyed by gestures more than native listeners due to their lack of full proficiency.",
            "startOffset": 23077,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 51487,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0105",
            "sentence": "On a behavioral level, this thus suggests that both native and non-native listeners attempt to integrate gestures with both clear and degraded speech.",
            "startOffset": 51337,
            "title": "Behavioral results - native & non-native listeners"
        },
        {
            "endOffset": 12390,
            "parents": [],
            "secId": "s0005",
            "sentence": "However, the neural mechanisms that support speech-gesture integration in non-native listeners in clear and degraded speech have never been investigated.",
            "startOffset": 12237,
            "title": "Introduction"
        },
        {
            "endOffset": 23865,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "Previous behavioral research on non-native degraded speech comprehension has been mostly tested in an auditory context, using only auditory semantic information in a verbal context as a modulating factor.",
            "startOffset": 23661,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 55101,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0275": {
                    "endOffset": 54906,
                    "startOffset": 54883
                }
            },
            "secId": "s0110",
            "sentence": "This is also in line with Obleser and Kotz (2011), who find that effortful semantic computation is more visible in less degraded signals, that is, when signal quality is good enough for semantic manipulations to have an effect on comprehension.",
            "startOffset": 54857,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 29426,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "We recruited highly-proficient non-native listeners with enough vocabulary knowledge of the words we presented.",
            "startOffset": 29315,
            "title": "The present study"
        },
        {
            "endOffset": 47563,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0085",
            "sentence": "We compared the ERPs of the four conditions time-locked to the onset of the video and averaged over all 23 native participants.",
            "startOffset": 47436,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 54538,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "We suggest that this shows an increase in neural resources that are required to resolve the speech signal and couple the semantic information conveyed by the gesture, resulting in an additive effect of both speech degradation and semantic incongruency of the gesture on the amplitude of the N400.",
            "startOffset": 54242,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 28668,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "We expected a similar N400 effect in degraded as in clear speech, resulting in a more negative N400 amplitude in response to mismatching compared to matching gestures.",
            "startOffset": 28501,
            "title": "The present study"
        },
        {
            "endOffset": 59907,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "As we did not observe an N400 effect in degraded speech, we suggest that non-native listeners might employ different neural processing strategies for semantic information than native listeners when speech is degraded.",
            "startOffset": 59690,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 15648,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "An alternative approach has been to investigate the temporal character of the brain mechanisms that support speech-gesture integration by measuring ERPs in the EEG signal.",
            "startOffset": 15477,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 62951,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Future studies could also test the current paradigm in a more sentential context, or whether similar results will hold when participants have a lower proficiency level, to test how a possible larger dependence on visual semantic information affects comprehension.",
            "startOffset": 62688,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 21412,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0275": {
                    "endOffset": 21243,
                    "startOffset": 21220
                }
            },
            "secId": "s0010",
            "sentence": "For example, Obleser and Kotz (2011) demonstrated that the N400 amplitude in response to low-cloze sentence-final words (indexing semantic integration load) decreased linearly with more signal degradation.",
            "startOffset": 21207,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 31427,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0030",
            "sentence": "All participants were students at Radboud University.",
            "startOffset": 31374,
            "title": "Participants"
        },
        {
            "endOffset": 44415,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "In general, non-native listeners showed similar behavioral results as natives regarding the differences in conditions.",
            "startOffset": 44297,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 31883,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0030",
            "sentence": "One participant from the Dutch participant group was excluded from analyses due to having excessive artifacts.",
            "startOffset": 31773,
            "title": "Participants"
        },
        {
            "endOffset": 58969,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "A direct comparison of the ERP waveforms of native and non-native listeners was not possible because of the many differences there could exist between these groups that are irrespective of the experimental manipulation, such as motivation (which might have been larger for the non-native group, as they completed a Dutch language proficiency test upon arrival).",
            "startOffset": 58608,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 29649,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "This would not be sufficient to study gestural enhancement of degraded speech comprehension.",
            "startOffset": 29557,
            "title": "The present study"
        },
        {
            "endOffset": 37065,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "All conditions consisted of 40 unique videos with unique verbs and gestures.",
            "startOffset": 36989,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 18412,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "For example, in an fMRI study, Holle et al. (2010) investigated which brain areas are responsive to speech-gesture integration, bimodal enhancement, and inverse effectiveness.",
            "startOffset": 18237,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 50657,
            "parents": [],
            "secId": "s0100",
            "sentence": "Natives, but not non-natives revealed an N400 effect in degraded speech.",
            "startOffset": 50585,
            "title": "Discussion"
        },
        {
            "endOffset": 32230,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0035",
            "sentence": "Participants are presented with 40 Dutch words and 20 nonwords.",
            "startOffset": 32167,
            "title": "LexTALE assessment"
        },
        {
            "endOffset": 38295,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "After each block, participants could take a self-paced break.",
            "startOffset": 38234,
            "title": "Procedure"
        },
        {
            "endOffset": 11545,
            "parents": [],
            "secId": "s0005",
            "sentence": "For example, a listener might see a speaker making a drinking gesture (i.e., a hand mimicking a glass that is moved towards the mouth) when she is asking whether someone wants a drink.",
            "startOffset": 11361,
            "title": "Introduction"
        },
        {
            "endOffset": 35394,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 35392,
                    "startOffset": 35368
                },
                "b0125": {
                    "endOffset": 35362,
                    "startOffset": 35343
                },
                "b0220": {
                    "endOffset": 35267,
                    "startOffset": 35226
                }
            },
            "secId": "s0040",
            "sentence": "This reveals that these gestures were potentially ambiguous without speech, which is mostly the case in spontaneous speech-gesture production (Krauss, Morrel-Samuels, & Colasante, 1991), and that they were to an extent dependent on speech to be disambiguated (Habets et al., 2011, see Drijvers & Ozy\u00fcrek, 2017).",
            "startOffset": 35083,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 45222,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "Lastly, we found a significant interaction between Noise-vocoding and Gesture, indicating that when speech was clear and the gesture matched the speech signal, non-native listeners showed a higher response accuracy (F(1, 22) = 82.91, p < .001, Wilks' Lambda = 0.21, \u03b72 = 0.79).",
            "startOffset": 44945,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 55217,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "Similarly, this is also the case for gestural information, which is partially ambigious without the speech context.",
            "startOffset": 55102,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 38830,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0050",
            "sentence": "The participants EEG was continuously recorded throughout the experiment from 32 AG-AgCI electrodes, of which 27 were mounted in a cap (actiCap) according to the 10\u201320 standard system, one was placed on the right mastoid for re-referencing and 4 were used for bipolar horizontal and vertical electrooculograms (EOG).",
            "startOffset": 38514,
            "title": "EEG data acquisition"
        },
        {
            "endOffset": 60430,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Native listeners on the other hand use and attempt to integrate the visual semantic information to immediately sharpen their perception to resolve the degraded speech signal, and can benefit more from this information than non-natives.",
            "startOffset": 60195,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 17572,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "They presented participants with videos where gestures were semantically congruent or incongruent, and where gesture and speech were presented either simultaneuos (SOA = 0), or the speech was delayed by 160 ms or 360 ms, and showed an N400 effect for the SOA 0 and SOA 160 conditions, but not the SOA 360 condition.",
            "startOffset": 17257,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 33419,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "The actress was visible from the knees up, wore neutrally colored clothing, and was standing in front of a unicolored background.",
            "startOffset": 33290,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 51070,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0105",
            "sentence": "Native and non-native listeners showed similar behavioral results and were more able to correctly identify a verb when speech was clear as compared to degraded, and when a gesture matched compared to mismatched speech.",
            "startOffset": 50852,
            "title": "Behavioral results - native & non-native listeners"
        },
        {
            "endOffset": 61231,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0120": {
                    "endOffset": 61037,
                    "startOffset": 60863
                },
                "b0155": {
                    "endOffset": 61037,
                    "startOffset": 60863
                },
                "b0165": {
                    "endOffset": 61037,
                    "startOffset": 60863
                },
                "b0200": {
                    "endOffset": 61229,
                    "startOffset": 61205
                },
                "b0210": {
                    "endOffset": 61229,
                    "startOffset": 61205
                },
                "b0215": {
                    "endOffset": 61133,
                    "startOffset": 61087
                },
                "b0315": {
                    "endOffset": 61037,
                    "startOffset": 60863
                },
                "b0330": {
                    "endOffset": 61037,
                    "startOffset": 60863
                },
                "b0355": {
                    "endOffset": 61037,
                    "startOffset": 60863
                },
                "b0360": {
                    "endOffset": 61037,
                    "startOffset": 60863
                }
            },
            "secId": "s0115",
            "sentence": "Right-hemisphere effects have been found in a range of studies that reported sensitivity of the right hemisphere during speech-gesture integration (especially in pSTS/MTG), (Green et al., 2009; Holle et al., 2010; Holler et al., 2014; Skipper, Goldin-Meadow, Nusbaum, & Small, 2009; Straube, Green, Weis, & Kircher, 2012; Willems et al., 2007, 2009), when semantic contexts are indirectly related (Kiefer, Weisbrod, Kern, Maier, & Spitzer, 1998) and when gestures were semantically more distant (i.e., mismatching) (Kelly et al., 2004, 2007).",
            "startOffset": 60689,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 27486,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "The sound in these videos was either clear or degraded.",
            "startOffset": 27431,
            "title": "The present study"
        },
        {
            "endOffset": 35837,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0300": {
                    "endOffset": 35835,
                    "startOffset": 35788
                }
            },
            "secId": "s0040",
            "sentence": "Noise-vocoding degrades the spectral content of the speech signal while pertaining the temporal envelope (Shannon, Zeng, Kamath, Wygonski, & Ekelid, 1995).",
            "startOffset": 35682,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 63956,
            "parents": [],
            "secId": "s0125",
            "sentence": "The authors declare no conflict of interest.",
            "startOffset": 63912,
            "title": "Conflict of interest"
        },
        {
            "endOffset": 26330,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "Therefore, to get a detailed insight into possible processing differences between native and non-native listeners during this semantic integration, an on-line method that can monitor the possible differences in neural integration is needed to investigate how the native language status of the listener influences the extent to which an iconic gesture is semantically integrated with clear and degraded speech.",
            "startOffset": 25921,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 27160,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0125": {
                    "endOffset": 26978,
                    "startOffset": 26887
                },
                "b0140": {
                    "endOffset": 26978,
                    "startOffset": 26887
                },
                "b0295": {
                    "endOffset": 26978,
                    "startOffset": 26887
                },
                "b0370": {
                    "endOffset": 26978,
                    "startOffset": 26887
                },
                "b0375": {
                    "endOffset": 26978,
                    "startOffset": 26887
                }
            },
            "secId": "s0020",
            "sentence": "In line with previous electrophysiological research on the neural integration of speech and iconic gestures (e.g., Habets et al., 2011; Holle & Gunter, 2007; Ozy\u00fcrek et al., 2007; Wu & Coulson, 2007a, 2007b), we focused on the N400 component to neurally assess differences in how visual semantic information is integrated with clear and degraded speech in native and non-native listeners.",
            "startOffset": 26772,
            "title": "The present study"
        },
        {
            "endOffset": 27606,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "All participants completed a behavioral cued-recall after each item that asked which verb they had heard in the videos.",
            "startOffset": 27487,
            "title": "The present study"
        },
        {
            "endOffset": 18236,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 18234,
                    "startOffset": 18190
                },
                "b0140": {
                    "endOffset": 18159,
                    "startOffset": 18139
                },
                "b0155": {
                    "endOffset": 18234,
                    "startOffset": 18190
                }
            },
            "secId": "s0010",
            "sentence": "Previous research has shown that visual semantic cues that are conveyed by iconic gestures can enhance clear speech comprehension when speech is ambiguous (Holle & Gunter, 2007) and when speech is degraded (Drijvers & Ozy\u00fcrek, 2017; Holle et al., 2010).",
            "startOffset": 17983,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 39207,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0285": {
                    "endOffset": 39150,
                    "startOffset": 39106
                }
            },
            "secId": "s0055",
            "sentence": "We analyzed the EEG data by using Fieldtrip (Oostenveld, Fries, Maris, & Schoffelen, 2011) a toolbox running under MATLAB (MathWorks, Natick, MA).",
            "startOffset": 39061,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 12236,
            "parents": [],
            "refoffsets": {
                "b0155": {
                    "endOffset": 12234,
                    "startOffset": 12155
                },
                "b0265": {
                    "endOffset": 12234,
                    "startOffset": 12155
                },
                "b9015": {
                    "endOffset": 12234,
                    "startOffset": 12155
                }
            },
            "secId": "s0005",
            "sentence": "So far, how the brain processes gestural information in the context of degraded speech has only been investigated in native listeners (Holle et al., 2010; Obermeier, Dolk, & Gunter, 2012b; Drijvers et al., in press).",
            "startOffset": 12020,
            "title": "Introduction"
        },
        {
            "endOffset": 24866,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "The results demonstrated that non-native listeners' comprehension was only aided when both semantic and acoustic information were available (e.g., in a sentence that was highly predictable and produced in clear speech).",
            "startOffset": 24647,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 39475,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "The data was segmented into epochs from -1 to 3.5 s relative to the onset of the videos.",
            "startOffset": 39387,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 11360,
            "parents": [],
            "secId": "s0005",
            "sentence": "During face-to-face communication, a listener's brain constantly integrates information from auditory inputs, such as speech, and visual inputs, such as iconic co-speech gestures.",
            "startOffset": 11181,
            "title": "Introduction"
        },
        {
            "endOffset": 30194,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "However, non-native listeners' electrophysiological responses might differ from natives when speech is degraded.",
            "startOffset": 30082,
            "title": "The present study"
        },
        {
            "endOffset": 46450,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "We observed a significant interaction between Noise-vocoding and Gesture, indicating that when speech was clear and the gesture matched the speech signal, non-native listeners were quicker to respond (F(1, 22) = 59.53, p < .001, Wilks' Lambda = 0.27, \u03b72 = 0.73).",
            "startOffset": 46188,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 52962,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "Here, listeners might need more neural effort or semantic unification operations to disambiguate both the degraded auditory cues and the visual semantic information that is conveyed by the gesture.",
            "startOffset": 52765,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 52147,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0100": {
                    "endOffset": 52145,
                    "startOffset": 52123
                }
            },
            "secId": "s0105",
            "sentence": "This could indicate that it is more difficult for them to resolve the remaining auditory cues and couple the semantic information that is conveyed by the gesture to the speech signal (similar to results on reduced speech, such as Ernestus et al. (2017)).",
            "startOffset": 51893,
            "title": "Behavioral results - native & non-native listeners"
        },
        {
            "endOffset": 62449,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0320": {
                    "endOffset": 62447,
                    "startOffset": 62421
                },
                "b0325": {
                    "endOffset": 62447,
                    "startOffset": 62421
                }
            },
            "secId": "s0115",
            "sentence": "This more extended network would also fit with the account that when speech processing becomes more taxing, additional neural resources are recruited to aid in comprehension (Skipper et al., 2006, 2007).",
            "startOffset": 62246,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 13008,
            "parents": [],
            "refoffsets": {
                "b0080": {
                    "endOffset": 12900,
                    "startOffset": 12851
                },
                "b0340": {
                    "endOffset": 12900,
                    "startOffset": 12851
                }
            },
            "secId": "s0005",
            "sentence": "Non-native listeners can also benefit from visual semantic cues from gestures (Dahl & Ludvigsen, 2014; Sueyoshi & Hardison, 2005), but this has been only studied behaviorally in clear speech and with low-proficient non-native listeners.",
            "startOffset": 12772,
            "title": "Introduction"
        },
        {
            "endOffset": 35570,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0045": {
                    "endOffset": 35494,
                    "startOffset": 35471
                }
            },
            "secId": "s0040",
            "sentence": "All auditory sound files were intensity-scaled to 70 dB, de-noised in Praat (Boersma & Weenink, 2015) and recombined with their corresponding video files in Adobe Premiere Pro.",
            "startOffset": 35394,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 12771,
            "parents": [],
            "refoffsets": {
                "b0055": {
                    "endOffset": 12769,
                    "startOffset": 12692
                },
                "b0250": {
                    "endOffset": 12769,
                    "startOffset": 12692
                },
                "b0380": {
                    "endOffset": 12769,
                    "startOffset": 12692
                }
            },
            "secId": "s0005",
            "sentence": "Previous studies have reported that non-native listeners can make use of auditory semantic-contextual cues (e.g., a previous sentence context) in adverse listening conditions to aid comprehension, but only when the auditory signal is of sufficient quality to facilitate access to semantic information (Bradlow & Alexander, 2007; Mayo, Florentine, & Buus, 1997; Zhang et al., 2016).",
            "startOffset": 12390,
            "title": "Introduction"
        },
        {
            "endOffset": 41583,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0065",
            "sentence": "Non-native listeners scored within the high-proficiency range, but performed lower than native listeners on the first LexTALE test (mean = 92.8 (SD = 4.86) for native listeners vs. mean = 76.4 (SD = 5.38) for non-native listeners, t(44) = 10.892, p = <.001) and in the second, adapted LexTALE test (mean = 96.41, (SD = 3.60), for native listeners vs. mean = 86.58, (SD = 5.32) for non-native listeners, t(44) = 7.34, p < .001).",
            "startOffset": 41156,
            "title": "Behavioral results - LexTALE"
        },
        {
            "endOffset": 27430,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 27320,
                    "startOffset": 27296
                }
            },
            "secId": "s0020",
            "sentence": "To this end, we presented native and highly proficient non-native listeners with videos of an actress uttering Dutch action verbs (see Drijvers & Ozy\u00fcrek, 2017), while she simultaneously made an iconic gesture that could either match or mismatch with the speech signal.",
            "startOffset": 27161,
            "title": "The present study"
        },
        {
            "endOffset": 13468,
            "parents": [],
            "secId": "s0005",
            "sentence": "Whereas non-native listeners might process gestural information more strongly in clear speech than natives, they might require more auditory cues to benefit from gestures in degraded speech than native listeners.",
            "startOffset": 13256,
            "title": "Introduction"
        },
        {
            "endOffset": 17112,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "This indicates that the brain is sensitive to the way gesture relates to speech, and that gesture is processed semantically.",
            "startOffset": 16988,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 63148,
            "parents": [],
            "secId": "s0120",
            "sentence": "Our data revealed that native and non-native listeners differ in the extent to which the semantic information from the gesture is coupled to the degraded speech signal on a neural level.",
            "startOffset": 62962,
            "title": "Conclusion"
        },
        {
            "endOffset": 50747,
            "parents": [],
            "secId": "s0100",
            "sentence": "Non-natives however, revealed a larger N400 effect in clear speech than native listeners.",
            "startOffset": 50658,
            "title": "Discussion"
        },
        {
            "endOffset": 22643,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0320": {
                    "endOffset": 22641,
                    "startOffset": 22564
                },
                "b0325": {
                    "endOffset": 22641,
                    "startOffset": 22564
                }
            },
            "secId": "s0010",
            "sentence": "This means that in response to degraded speech, the N400 amplitude might be more enhanced compared to clear speech, as a listener might recruit more neural resources when speech is degraded, such as visual semantic information that is conveyed by gestures to try to resolve the auditory input (in line with Skipper, Nusbaum, & Small, 2006; Skipper, Wassenhove, Nusbaum, & Steven, 2007).",
            "startOffset": 22257,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 57416,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "This might have resulted in a similar N400 amplitude of the degraded conditions and the clear speech and mismatching gesture condition, or could be explained by a ceiling effect.",
            "startOffset": 57238,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 38015,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "After a short delay (1500 ms) participants were presented with a cued-verb recall task and asked to identify which verb (out of four alternatives: correct answer, phonological competitor, semantic competitor, unrelated answer) they heard in the video by pressing a 4-button box.",
            "startOffset": 37737,
            "title": "Procedure"
        },
        {
            "endOffset": 41751,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0065",
            "sentence": "The second test assessed their knowledge about the words we used as stimuli, and revealed that they were highly familiar with them, reaching almost native-like levels.",
            "startOffset": 41584,
            "title": "Behavioral results - LexTALE"
        },
        {
            "endOffset": 56542,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "These results seem in line with theoretical explanations of why differences between native and non-native listeners arise under adverse listening conditions.",
            "startOffset": 56385,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 43602,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "We found a significant main effect of Gesture (F(1, 22) = 69.20, p = <.001, Wilks' Lambda = 0.24, \u03b72 = 0.76), indicating that when the gesture matched with the speech signal native listeners answered more quickly.",
            "startOffset": 43389,
            "title": "Native listeners"
        },
        {
            "endOffset": 13989,
            "parents": [],
            "secId": "s0005",
            "sentence": "To investigate this, the present study uses behavioral measures and event-related potentials (ERPs) as a measure of online neural semantic integration to investigate how native and non-native listeners integrate gestures with clear and degraded speech.",
            "startOffset": 13737,
            "title": "Introduction"
        },
        {
            "endOffset": 36278,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 36276,
                    "startOffset": 36252
                }
            },
            "secId": "s0040",
            "sentence": "Since our previous experiment (see Drijvers & Ozy\u00fcrek, 2017) identified a 6-band noise-vocoding level as the optimal range in which iconic gestures can enhance degraded speech comprehension the most, this was also the speech degradation level that was used in this experiment (see Drijvers & Ozy\u00fcrek, 2017).",
            "startOffset": 35971,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 29314,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "For non-native listeners we expected similar behavioral results for all conditions due to their high proficiency.",
            "startOffset": 29201,
            "title": "The present study"
        },
        {
            "endOffset": 48029,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0085",
            "sentence": "For native listeners, we observed a significant difference between the clear-match and the clear-mismatch condition (clear-mismatch > clear-match, p < .001), the degraded-match and the degraded-mismatch condition (degraded-mismatch > degraded-match, p < .05), between the clear-match condition and degraded-match condition (clear-match < degraded-match, p < .001) and the clear-mismatch and degraded-mismatch condition (clear-mismatch < degraded-mismatch, p < .001).",
            "startOffset": 47563,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 57806,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "In line with this interpretation of our data, we also observed a smaller N400 effect for natives in degraded compared to clear speech.",
            "startOffset": 57672,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 36988,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "In total, four conditions were created for this experiment: a clear speech + matching gesture condition ('clear-match', e.g., 'to eat' in clear speech combined with a co-speech gesture for 'to eat'), a clear speech + mismatching gesture condition ('clear-mismatch', e.g., 'to call' in clear speech combined with a mismatching co-speech gesture for 'to drive'), a degraded speech + matching gesture condition ('degraded-match', e.g., 'to mix' in degraded speech combined with a matching co-speech gesture for 'mixing') and a degraded speech + mismatching gesture condition ('degraded-mismatch', e.g., 'to turn' in degraded speech with a mismatching co-speech gesture for 'salting') (see Fig. 1 for an overview).",
            "startOffset": 36278,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 48195,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0085",
            "sentence": "Fig. 3 shows the grand average event-related potentials for all four conditions, as well as the topographical plots of the N400 effects in clear and degraded speech.",
            "startOffset": 48030,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 31374,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0030",
            "sentence": "All participants gave informed written consent before the start of the experiment and received a financial compensation for participation.",
            "startOffset": 31236,
            "title": "Participants"
        },
        {
            "endOffset": 43139,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "Bonferroni corrected post-hoc analyses revealed a difference between clear-match and clear-mismatch t(22) = 6.67, pbon < 0.001, between degraded-match and degraded-mismatch t(22) = 11.12, pbon < .001, between clear-match and degraded-match t(22) = 7.89, pbon < .001 and between clear-mismatch and degraded-mismatch t(22) = 12.42, pbon < .001 (see Fig. 2).",
            "startOffset": 42784,
            "title": "Native listeners"
        },
        {
            "endOffset": 17256,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0125": {
                    "endOffset": 17146,
                    "startOffset": 17126
                }
            },
            "secId": "s0010",
            "sentence": "For example, Habets et al. (2011) investigated the degree of asynchrony in speech and gesture onsets that are optimal for semantic integration.",
            "startOffset": 17113,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 34926,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "These mismatching gestures were created by dividing the list of verbs in the mismatch conditions in two lists, and combining the verbs on the first list with the gesture corresponding to a verb on the second list, and vice versa (e.g., a verb on the first list ('drink') would be coupled with a verb on the second list ('salt'), so the actress would utter the word 'drink' while making a salting gesture).",
            "startOffset": 34521,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 39775,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "One participant from the Dutch participant group was excluded from analyses due to having excessive artifacts.",
            "startOffset": 39665,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 42503,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "We also found an effect of Gesture (F(1, 22) = 128.87, p = <.001, Wilks' Lambda = 0.146, \u03b72 = 0.85), indicating that when the gesture matched the speech signal, native listeners were more able to correctly identify the verb.",
            "startOffset": 42279,
            "title": "Native listeners"
        },
        {
            "endOffset": 48310,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0085",
            "sentence": "Degraded-mismatch elicited the largest N400 amplitude, followed by degraded-match, clear-mismatch and clear-match.",
            "startOffset": 48196,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 60194,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "If however the remaining auditory cues are not reliable enough, they cannot benefit from these semantic cues.",
            "startOffset": 60085,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 58000,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Similarly, this result suggested that the neural processing of semantic integration already suffered from having less auditory cues present to map the semantic information from the gestures to.",
            "startOffset": 57807,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 16141,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0075": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0125": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0140": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0185": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0200": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0270": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0365": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0370": {
                    "endOffset": 16059,
                    "startOffset": 15860
                },
                "b0375": {
                    "endOffset": 16059,
                    "startOffset": 15860
                }
            },
            "secId": "s0010",
            "sentence": "Previous studies on the neural integration of iconic gestures and clear speech in native listeners (Cornejo et al., 2009; Habets, Kita, Shao, Ozyurek, & Hagoort, 2011; Holle & Gunter, 2007; Kelly, Kravitz, & Hopkins, 2004; Kelly et al., 1999; Obermeier et al., 2011; Wu & Coulson, 2005, 2007a, 2007b) have focused on the N400 component to assess differences in semantic processing.",
            "startOffset": 15760,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 15759,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "ERPs can be seen as deflections in voltage that are measured and recorded from electrodes placed on the scalp.",
            "startOffset": 15649,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 24646,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0055": {
                    "endOffset": 24484,
                    "startOffset": 24456
                }
            },
            "secId": "s0015",
            "sentence": "For example, in a behavioral study, Bradlow and Alexander (2007) presented native and non-native listeners with sentences in which the final word would either be highly predictable or not and produced in plain or clear speech.",
            "startOffset": 24420,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 30081,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0080": {
                    "endOffset": 29996,
                    "startOffset": 29974
                }
            },
            "secId": "s0020",
            "sentence": "Based on previous research showing that non-natives might make more use of gestural context (e.g., Dahl & Ludvigsen, 2014) we expected that this N400 effect might be stronger in non-natives than in natives.",
            "startOffset": 29875,
            "title": "The present study"
        },
        {
            "endOffset": 40519,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "In each of these clusters, the t-statistics were summed in order to calculate the cluster-level statistics.",
            "startOffset": 40412,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 57672,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "This might have masked the actual comprehension difficulties the listeners had when they watched the video.",
            "startOffset": 57565,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 42783,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "We found a significant interaction between Noise-vocoding and Gesture (F(1, 22) = 112.20, p = <.001, Wilks' Lambda = 0.164, \u03b72 = 0.83), indicating that when the speech signal was clear and the gesture matched the speech signal, participants demonstrated higher response accuracy.",
            "startOffset": 42504,
            "title": "Native listeners"
        },
        {
            "endOffset": 39386,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "First, we re-referenced the EEG data offline to the average of the right and left mastoid and filtered the data with a high-pass filter at 0.01 Hz and a low pass filter at 35 Hz.",
            "startOffset": 39208,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 45639,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "Post-hoc analyses (Bonferroni corrected) showed revealed no significant difference in response accuracy between clear-match and clear-mismatch, t(22) = -0.92, p = .367, but did show significant differences between degraded-match and degraded-mismatch, t(22) = \u22129.55, p < .001, between clear-match and degraded-match t(22) = \u22126.74, p < .001, and between clear-mismatch and degraded-mismatch, t(22) = \u221215.29, p < .001.",
            "startOffset": 45223,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 53909,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "We expected that, to some extent, gestures could therefore elicit predictions about the degraded word, which, in turn, could have enhanced degraded speech comprehension, resulting in the recruitment of more neural resources compared to clear speech.",
            "startOffset": 53660,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 61706,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "A similar pattern is observed in the N400 effect in degraded speech for native listeners, where we observed a widespread negativity over both left and right lateralized electrodes.",
            "startOffset": 61526,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 52764,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "When speech was degraded we observed a similar pattern with more negative N400 amplitudes than in clear speech, suggesting more neural resources were required to integrate gestures when speech was degraded.",
            "startOffset": 52558,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 20692,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "In the auditory domain, previous ERP studies have mostly focused on degraded speech comprehension in an auditory semantic context (e.g., a previous sentence context).",
            "startOffset": 20526,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 32886,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0035",
            "sentence": "Again, this version consisted of 40 real words from the main experiment and 20 nonwords.",
            "startOffset": 32798,
            "title": "LexTALE assessment"
        },
        {
            "endOffset": 35970,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "The speech signal then remains intelligible to a certain extent, with more bands corresponding to a more intelligible speech signal.",
            "startOffset": 35838,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 22075,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "The question remains however whether the neural resources that are needed to integrate a word with semantic information are similarly modulated by the imposed perceptual load of degraded speech when visual semantic context (e.g., iconic gestures) instead of auditory semantic context is provided.",
            "startOffset": 21779,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 55344,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "In Fig. 3A and B, a possible latency of the N400-effect can be observed when comparing the effect in degraded and clear speech.",
            "startOffset": 55217,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 22887,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0125": {
                    "endOffset": 22827,
                    "startOffset": 22808
                }
            },
            "secId": "s0010",
            "sentence": "Furthermore, the N400 effect in degraded speech might be smaller than in clear speech, due to the fact that gestures also need speech for their disambiguation (see Habets et al., 2011), and speech quality is diminished when speech is degraded.",
            "startOffset": 22644,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 33847,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "The actress always produced an iconic co-speech gesture that could either match or mismatch with the spoken verb (e.g., the verb 'drive' and a driving gesture in the match conditions, and the verb 'eat' with a mixing gesture in the mismatch conditions, see Fig. 1A).",
            "startOffset": 33581,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 18672,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "Simultaneously, the actor in these videos would either make an accompanying iconic gesture or no gesture.",
            "startOffset": 18567,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 25747,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0130": {
                    "endOffset": 25426,
                    "startOffset": 25407
                }
            },
            "secId": "s0015",
            "sentence": "In line with this, another audiovisual behavioral study by Hazan et al. (2006) demonstrated that non-native listeners effectively incorporate and use visual cues from visible speech that are related to phonological features in the auditory signal to enhance speech comprehension in noise, and that increasing auditory proficiency is linked to an increased use of visual cues by non-native listeners.",
            "startOffset": 25348,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 23457,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0080": {
                    "endOffset": 23455,
                    "startOffset": 23376
                },
                "b0240": {
                    "endOffset": 23455,
                    "startOffset": 23376
                },
                "b0340": {
                    "endOffset": 23455,
                    "startOffset": 23376
                }
            },
            "secId": "s0015",
            "sentence": "Behavioral studies have shown that iconic co-speech gestures can enhance non-native language comprehension and non-native language learning (e.g., Dahl & Ludvigsen, 2014; Macedonia & Kriegstein, 2012; Sueyoshi & Hardison, 2005).",
            "startOffset": 23229,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 39664,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "On average, we excluded 8,1% of the trials for each participant (13/160).",
            "startOffset": 39591,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 59144,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "For example, post-hoc analyses of the N1/P2 complex at the start of the video revealed differences between the groups that could not be explained by stimulus characteristics.",
            "startOffset": 58970,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 51892,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0105",
            "sentence": "However, although the behavioral patterns of both groups look similar with regard to the differences between the conditions, non-native listeners demonstrated lower overall accuracy scores and slower reaction times in the degraded conditions than in the clear conditions when compared to natives.",
            "startOffset": 51596,
            "title": "Behavioral results - native & non-native listeners"
        },
        {
            "endOffset": 32166,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0235": {
                    "endOffset": 32164,
                    "startOffset": 32139
                }
            },
            "secId": "s0035",
            "sentence": "Before the main experiment, the Dutch proficiency level of all participants was assessed by the Dutch version of the Lexical Test for Advanced Learners of English (LexTALE), a vocabulary test using non-speeded visual lexical decision (Lemh\u00f6fer & Broersma, 2012).",
            "startOffset": 31904,
            "title": "LexTALE assessment"
        },
        {
            "endOffset": 29556,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "Low proficient participants would not recognize all of the verbs, and possibly be able to only pick up information from gestures.",
            "startOffset": 29427,
            "title": "The present study"
        },
        {
            "endOffset": 35681,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "From every cleaned audio-file, a 6-band noise-vocoded version was created by using a custom-made Praat script.",
            "startOffset": 35571,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 37212,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "Upon arrival, participants first completed a consent form and participated in the LexTALE test before they were fitted with an EEG cap.",
            "startOffset": 37077,
            "title": "Procedure"
        },
        {
            "endOffset": 38233,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "The constraint on this randomization was that a condition could not be presented more than twice in a row.",
            "startOffset": 38127,
            "title": "Procedure"
        },
        {
            "endOffset": 37639,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "Stimuli were presented full screen on a 1650 \u00d7 1080 monitor by using Presentation software (version 16.4; Neurobehavioral Systems, Inc.) Participants were explained that the videos would contain a girl who would utter a Dutch action verb and asked to attentively watch and listen to the stimuli.",
            "startOffset": 37344,
            "title": "Procedure"
        },
        {
            "endOffset": 60688,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Although differences in the distribution of the N400 component should be carefully made on the basis of ERP scalp topographies, we observed a more right-lateralized topography of the N400 effect in clear speech for non-native as compared to native listeners.",
            "startOffset": 60430,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 40411,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "All adjacent data points that exceeded a pre-set threshold of 5% were grouped into clusters.",
            "startOffset": 40319,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 21779,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "By diminishing the sensory input, the neural system might rely more on signal-driven expectancies than contextual information.",
            "startOffset": 21653,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 48524,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0085",
            "sentence": "In the clear speech conditions, the N400 effect was most pronounced over central-parietal electrodes, but in the degraded conditions, this effect was more widespread over left and right temporoparietal electrodes.",
            "startOffset": 48311,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 44274,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "Bonferroni corrected post-hoc analyses revealed no significant difference between clear-match and clear-mismatch, t(22) = \u22120.96, pbon = 0.348, but did show significant differences between degraded-match and degraded-mismatch, t(22) = \u22127.80, pbon < .001, between clear-match and degraded-match t(22) = \u22126.97, pbon < .001, and between clear-mismatch and degraded-mismatch, t(22) = \u22128.73, pbon < .001.",
            "startOffset": 43876,
            "title": "Native listeners"
        },
        {
            "endOffset": 60084,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "One possibility is that non-native listeners first try to resolve the degraded auditory cues and recruit more visual information when resolving the degraded cues is too taxing.",
            "startOffset": 59908,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 38352,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "All participants completed the experiment within 30 min.",
            "startOffset": 38296,
            "title": "Procedure"
        },
        {
            "endOffset": 42278,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "We observed a significant effect of Noise-vocoding, indicating that when the speech signal was clear, native listeners' response accuracy was higher than when the speech was degraded (F(1, 22) = 140.95, p < .001, Wilks' Lambda = 0.135, \u03b72 = 0.87).",
            "startOffset": 42031,
            "title": "Native listeners"
        },
        {
            "endOffset": 50584,
            "parents": [],
            "secId": "s0100",
            "sentence": "Even though native and non-native listeners demonstrated similar behavioral results, our EEG results suggested that native and non-native listeners neurally integrate speech with gestures differently in both clear and degraded speech.",
            "startOffset": 50350,
            "title": "Discussion"
        },
        {
            "endOffset": 34520,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "Since our videos showed the face of the actress and we could therefore not recombine a mismatching auditory track to a video to create the mismatch condition, we asked the actress to utter a verb and produce a mismatching gesture with it.",
            "startOffset": 34282,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 57237,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "In turn, this may have caused a limited benefit from visual information for comprehension, especially when the degraded auditory cues were not reliable enough to couple the visual semantic information to or for the visual information to boost comprehension of the degraded auditory cues.",
            "startOffset": 56950,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 16987,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0075": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0125": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0200": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0205": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0210": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0295": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0305": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0370": {
                    "endOffset": 16847,
                    "startOffset": 16627
                },
                "b0375": {
                    "endOffset": 16847,
                    "startOffset": 16627
                }
            },
            "secId": "s0010",
            "sentence": "Previous ERP studies on gesture processing have shown modulations of the N400 amplitude in mismatch paradigms (e.g., Cornejo et al., 2009; Habets et al., 2011; Kelly & Lee, 2012; Kelly, Ward, Creigh, & Bartolotti, 2007; Kelly et al., 2004; Ozy\u00fcrek, Willems, Kita, & Hagoort, 2007; Sheehan, Namy, & Mills, 2007; Wu & Coulson, 2007a, 2007b), with more negative N400 amplitudes in response to speech that was presented with a mismatching gesture as compared to a matching gesture.",
            "startOffset": 16510,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 39590,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "We used a baseline window of -0.4 s to -0.2 s. Artifacts were removed by using a semi-automatic rejection routine.",
            "startOffset": 39476,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 46851,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "Bonferroni corrected post-hoc analyses revealed no significant difference between clear-match and clear-mismatch, t(22) = \u22120.71, pbon = 0.483, but did show significant differences between degraded-match and degraded-mismatch, t(22) = \u22128.576, pbon < .001, between clear-match and degraded-match t(22) = \u22128.48, pbon < .001, and between clear-mismatch and degraded-mismatch, t(22) = \u221210.76, pbon < .001.",
            "startOffset": 46451,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 56819,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0055": {
                    "endOffset": 56817,
                    "startOffset": 56687
                },
                "b0060": {
                    "endOffset": 56817,
                    "startOffset": 56687
                },
                "b0105": {
                    "endOffset": 56817,
                    "startOffset": 56687
                },
                "b0115": {
                    "endOffset": 56817,
                    "startOffset": 56687
                },
                "b0250": {
                    "endOffset": 56817,
                    "startOffset": 56687
                },
                "b0280": {
                    "endOffset": 56817,
                    "startOffset": 56687
                }
            },
            "secId": "s0115",
            "sentence": "Possibly, non-native listeners cannot fully make use of the semantical cues of the gesture when the auditory cues are too difficult to resolve (Bradlow & Alexander, 2007; Bradlow & Bent, 2002; Gat & Keith, 1978; Golestani et al., 2009; Mayo et al., 1997; Oliver et al., 2012).",
            "startOffset": 56543,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 13255,
            "parents": [],
            "secId": "s0005",
            "sentence": "It remains unclear whether and how the semantic cues from iconic co-speech gestures can influence the neural processing of degraded speech comprehension in highly-proficient non-native listeners with sufficient vocabulary knowledge of a language.",
            "startOffset": 13009,
            "title": "Introduction"
        },
        {
            "endOffset": 21652,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0335": {
                    "endOffset": 21487,
                    "startOffset": 21455
                }
            },
            "secId": "s0010",
            "sentence": "In line with this, a similar EEG study by Strau\u00df, Kotz, and Obleser (2013) on the influence of expectancies under degraded speech comprehension proposed that an adverse listening condition might narrow expectancies about the speech signal.",
            "startOffset": 21413,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 62245,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Especially when speech is degraded, the dynamic reverberating circuits involved between (L)IFG and pSTS/MTG might be more widespread to recruit more top-down information to enhance degraded speech comprehension and facilitate unification of the two input streams.",
            "startOffset": 61982,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 59425,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "However, we did compare the N400 effects in the two groups, and found a larger N400 effect in clear speech for non-native compared to native listeners, and a larger N400 effect in degraded speech for native listeners (due to the absence of an N400 effect in non-native listeners).",
            "startOffset": 59145,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 31772,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0030",
            "sentence": "The German participants ('non-native listeners') were recruited on the basis of the following criteria: They had lived or studied in the Netherlands for at least 1 year, had to use Dutch regularly (minimally once per week) for their studies and/or their personal lives, and acquired Dutch after age 12 (range: 12\u201323, mean age = 18.7, SD = 2.5).",
            "startOffset": 31428,
            "title": "Participants"
        },
        {
            "endOffset": 34282,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0125": {
                    "endOffset": 34266,
                    "startOffset": 34247
                }
            },
            "secId": "s0040",
            "sentence": "The preparation of these gestures always started 120 ms after video onset, the stroke of the gesture started on average at 550 ms, gesture retraction at 1380 ms, and gesture ended at 1780 ms. Speech onset was on average at 680 ms, which means that stroke onset started 130 ms before speech onset, maximizing the overlap between the meaningful part of the gesture and speech for mutual comprehension (Habets et al., 2011) (see Fig. 1C).",
            "startOffset": 33847,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 28971,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "However, we predicted this N400 effect to be smaller in degraded speech, because semantically coupling degraded speech with gestures will be more effortful due to the fact that the diminished auditory input will not always be resolved by gestures, especially not when the gesture mismatches the signal.",
            "startOffset": 28669,
            "title": "The present study"
        },
        {
            "endOffset": 33289,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "All videos were recorded with a JVC GY-HM100 camcorder and had an average length of 2 s (see Fig. 1B).",
            "startOffset": 33187,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 39965,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "To calculate the event-related potential, the time-locked average (time-locked to video onset) over all remaining trials was computed separately for the four conditions for each participant.",
            "startOffset": 39775,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 32367,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0035",
            "sentence": "Nonwords were nonsense strings created either by changing a number of letters in an existing word, or by recombining existing morphemes.",
            "startOffset": 32231,
            "title": "LexTALE assessment"
        },
        {
            "endOffset": 53350,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0005": {
                    "endOffset": 53348,
                    "startOffset": 53284
                },
                "b0275": {
                    "endOffset": 53348,
                    "startOffset": 53284
                },
                "b0335": {
                    "endOffset": 53348,
                    "startOffset": 53284
                }
            },
            "secId": "s0110",
            "sentence": "Interestingly, previous research on auditory degraded speech comprehension has reported a reduced N400 amplitude when auditory target words were (increasingly) degraded as compared to clear and when they were presented in a low-cloze probability context (e.g., when semantic expectations about the upcoming word are low) (Aydelott et al., 2006; Obleser & Kotz, 2011; Strau\u00df et al., 2013).",
            "startOffset": 52962,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 14874,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0015": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0020": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0025": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0030": {
                    "endOffset": 14872,
                    "startOffset": 14687
                },
                "b0035": {
                    "endOffset": 14872,
                    "startOffset": 14687
                },
                "b0040": {
                    "endOffset": 14872,
                    "startOffset": 14687
                },
                "b0090": {
                    "endOffset": 14872,
                    "startOffset": 14687
                },
                "b0140": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0150": {
                    "endOffset": 14872,
                    "startOffset": 14687
                },
                "b0160": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0165": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0175": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0185": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0190": {
                    "endOffset": 14623,
                    "startOffset": 14590
                },
                "b0195": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0270": {
                    "endOffset": 14493,
                    "startOffset": 14203
                },
                "b0290": {
                    "endOffset": 14526,
                    "startOffset": 14513
                },
                "b0345": {
                    "endOffset": 14872,
                    "startOffset": 14687
                }
            },
            "secId": "s0010",
            "sentence": "There is ample evidence from both behavioral and neuroimaging studies that native listeners process and integrate gestures with clear speech (e.g. Beattie & Shovelton, 1999a, 1999b; Beattie & Shovelton, 2002; Holle & Gunter, 2007; Holler, Kelly, Hagoort, & Ozyurek, 2010; Holler, Shovelton, & Beattie, 2009; Holler et al., 2014; Kelly, Barr, Church, & Lynch, 1999; Kelly, Healey, \u00d6zy\u00fcrek, & Holler, 2015; Obermeier, Holle, & Gunter, 2011; see for a review, \u00d6zy\u00fcrek, 2014), even when the gesture is irrelevant for the listeners' task (Kelly, Creigh, & Bartolotti, 2010), or when the gesture has no semantic content (beat gestures) (Biau & Soto-Faraco, 2013, 2015; Biau, Torralba, Fuentemilla, de Diego Balaguer, & Soto-Faraco, 2015; Dimitrova, Chu, Wang, \u00d6zy\u00fcrek, & Hagoort, 2016; Holle et al., 2012; Wang & Chu, 2013).",
            "startOffset": 14056,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 26771,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "Here, we measure the brain's electrophysiological response to the speech and gesture videos by focusing on ERPs in the EEG signal, to exploit the excellent temporal resolution this method offers.",
            "startOffset": 26576,
            "title": "The present study"
        },
        {
            "endOffset": 62687,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "In future work, we aim to address these questions by including a baseline condition where there is no gesture present, to investigate whether the semantic information from the gesture enhances recognition depending on semantic congruency.",
            "startOffset": 62449,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 28216,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "On an electrophysiological level, we expected that integrating gestures with degraded speech is more effortful and requires more neural resources than in clear speech because there are less auditory cues available.",
            "startOffset": 28002,
            "title": "The present study"
        },
        {
            "endOffset": 46187,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "We observed a similar pattern in reaction times as in response accuracy: We observed a significant main effect of Noise-vocoding (F(1, 22) = 104.554, p < .001, Wilks' Lambda = 0.174, \u03b72 = 0.82), indicating that non-native listeners were quicker to respond when the speech signal was clear and a significant main effect of Gesture, indicating that when the gesture matched the speech signal, non-native listeners responded quicker than when the gesture mismatched with the speech signal (F(1, 22) = 53.42, p < .001, Wilks' Lambda = 0.29, \u03b72 = 0.70).",
            "startOffset": 45639,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 49062,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0090",
            "sentence": "In non-native listeners, we observed a significant difference between clear-match and clear-mismatch (clear-mismatch > clear-match, p < .001), but not between degraded-match and degraded-mismatch (p = .16).",
            "startOffset": 48856,
            "title": "EEG data - non-native listeners"
        },
        {
            "endOffset": 25347,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0055": {
                    "endOffset": 25345,
                    "startOffset": 25255
                },
                "b0115": {
                    "endOffset": 25345,
                    "startOffset": 25255
                },
                "b0280": {
                    "endOffset": 25345,
                    "startOffset": 25255
                },
                "b0380": {
                    "endOffset": 25345,
                    "startOffset": 25255
                }
            },
            "secId": "s0015",
            "sentence": "One of the explanations for this difference between native and non-native listeners is that non-native listeners might not be able to use semantic contextual information to resolve the information loss at the phoneme level when the signal clarity was insufficient (e.g.,Bradlow & Alexander, 2007; Golestani et al., 2009; Oliver et al., 2012; Zhang et al., 2016).",
            "startOffset": 24985,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 32489,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0035",
            "sentence": "Only German participants with a proficiency level of 67.5% and higher were allowed to participate in the main experiment.",
            "startOffset": 32368,
            "title": "LexTALE assessment"
        },
        {
            "endOffset": 38491,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "After the experiment, participants filled in the adapted version of the LexTALE to test their knowledge of the verbs used in these videos.",
            "startOffset": 38353,
            "title": "Procedure"
        },
        {
            "endOffset": 12019,
            "parents": [],
            "refoffsets": {
                "b0095": {
                    "endOffset": 12017,
                    "startOffset": 11954
                },
                "b9015": {
                    "endOffset": 12017,
                    "startOffset": 11954
                }
            },
            "secId": "s0005",
            "sentence": "This semantic information can affect the processing of speech comprehension in normal and adverse listening conditions, such as in degraded speech (Drijvers & Ozy\u00fcrek, 2017; Drijvers, Ozyurek, & Jensen, in press).",
            "startOffset": 11806,
            "title": "Introduction"
        },
        {
            "endOffset": 16397,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "The N400 is a negative-going ERP component between 300 and 600 ms. that peaks around 400 ms. The amplitude of the N400 is interpreted to reflect the ease of semantic integration and the extent to which neural resources are needed to integrate information.",
            "startOffset": 16142,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 32797,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0035",
            "sentence": "After the main EEG experiment (described below), participants were presented with an adapted version of the LexTALE to assees their knowledge of the specific verbs that we used in the main experiment.",
            "startOffset": 32597,
            "title": "LexTALE assessment"
        },
        {
            "endOffset": 41117,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0245": {
                    "endOffset": 41115,
                    "startOffset": 41091
                }
            },
            "secId": "s0055",
            "sentence": "Clusters that fell in the highest or lowest 2.5th percentile of the distribution were considered significant (see Maris & Oostenveld, 2007).",
            "startOffset": 40977,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 58608,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Note that a mismatching gesture in degraded speech can possibly also have a deleterious effect, when the visual information was difficult to integrate with the remaining auditory cues and the semantic information did not aid resolving the auditory cues.",
            "startOffset": 58355,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 51336,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0105",
            "sentence": "Reaction times revealed a similar pattern, but no difference in reaction times was observed between clear speech and a matching compared to a mismatching gesture for both native and non-native listeners, which is possibly due to a ceiling effect in both conditions.",
            "startOffset": 51071,
            "title": "Behavioral results - native & non-native listeners"
        },
        {
            "endOffset": 20336,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "Second, Holle et al. (2010) have presented gestures in head-occluded conditions, and not in a context where all visual articulators are visible to participants.",
            "startOffset": 20176,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 40739,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "Then, a Monte-Carlo permutation distribution was created by randomly assigning a participant\u2019s average to one of the two conditions (1000 times) and calculating the largest cluster-level statistic for every permutation.",
            "startOffset": 40520,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 52202,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0105",
            "sentence": "Our EEG results provided more evidence for this claim.",
            "startOffset": 52148,
            "title": "Behavioral results - native & non-native listeners"
        },
        {
            "endOffset": 59689,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "This revealed that in clear speech, non-native listeners possibly recruit the visual semantic information more than native listeners, which is possibly due to the fact that they pay more attention to gestures when they are unsure about their language proficiency.",
            "startOffset": 59426,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 38126,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "The order of the stimuli was pseudo-randomized for all participants and presented in four blocks of 40 trials.",
            "startOffset": 38016,
            "title": "Procedure"
        },
        {
            "endOffset": 19183,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 18961,
                    "startOffset": 18937
                }
            },
            "secId": "s0010",
            "sentence": "Similarly, in a recent experiment (Drijvers & Ozy\u00fcrek, 2017), we presented participants with videos with varying levels of visual information: videos could either contain a speaker with her lips blurred, a speaker with visible speech, or a speaker with visual speech and a gesture.",
            "startOffset": 18902,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 31082,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0030",
            "sentence": "Twenty-four Dutch participants (mean age = 21.6, SD = 1.97, 9 males) and twenty-three German advanced learners of Dutch (mean age = 22.4, SD = 2.35, 8 males) participated in this experiment.",
            "startOffset": 30892,
            "title": "Participants"
        },
        {
            "endOffset": 33059,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 33058,
                    "startOffset": 33031
                }
            },
            "secId": "s0040",
            "sentence": "The materials in this experiment are partially based on a subset of pretested stimuli which are described in more detail in Drijvers and Ozy\u00fcrek (2017).",
            "startOffset": 32907,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 50206,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0095",
            "sentence": "We observed a larger N400 effect in clear speech for non-native listeners as compared to native listeners (p < .05) and a larger N400 effect for native as compared to non-native listeners in degraded speech (p < .05), which was driven by the absence of an N400 effect in the non-native listeners group.",
            "startOffset": 49904,
            "title": "EEG data - native versus non-native listeners"
        },
        {
            "endOffset": 56176,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Similar to native listeners, non-native listeners showed a more negative N400 amplitude in clear speech for mismatching than matching gestures.",
            "startOffset": 56033,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 23661,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "However, up to date, there are no studies on the neural correlates of how visual semantic cues that are conveyed by gestures might enhance clear or degraded speech comprehension for non-native listeners.",
            "startOffset": 23458,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 53659,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "Note that this visual semantic context was not completely unambiguous, as both the gesture and the speech could mutually disambiguate each other.",
            "startOffset": 53514,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 49570,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0090",
            "sentence": "The topographical plots revealed that the N400 effect in clear speech extends over central-parietal as well as right lateralized electrodes, unlike what was observed in natives.",
            "startOffset": 49393,
            "title": "EEG data - non-native listeners"
        },
        {
            "endOffset": 19733,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "The results revealed that listeners benefit more from having two visual articulators (i.e., visual speech and iconic gestures) present as compared to one (i.e., visible speech only), and that this benefit was largest at a moderate vocoding level, where listeners can still benefit from both the phonological cues from visible speech and semantic cues from iconic gestures to disambiguate the speech.",
            "startOffset": 19334,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 20175,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0010",
            "sentence": "However, although Holle et al. (2010) have demonstrated the spatial neural correlates of speech-gesture integration in adverse listening conditions, it remains unclear what the online temporal neural correlates are of how the semantic information from iconic gestures enhances the comprehension of degraded speech, and whether matching and mismatching gestures have an effect on the N400 amplitude in clear and degraded listening conditions.",
            "startOffset": 19734,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 56385,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "In degraded speech, non-native listeners revealed no difference between matching and mismatching gestures, nor did these N400 amplitudes differ from the N400 amplitude of mismatching gestures in clear speech.",
            "startOffset": 56177,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 63679,
            "parents": [],
            "secId": "s0120",
            "sentence": "While both native and non-native listeners use more neural resources to disambiguate the degraded speech signal, non-native listeners were more hindered in their ability to neurally couple the semantic information from the gesture to degraded auditory cues, possibly because they need more auditory cues to facilitate access to gestural information.",
            "startOffset": 63330,
            "title": "Conclusion"
        },
        {
            "endOffset": 31235,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0030",
            "sentence": "All participants were right-handed and reported no language impairments, normal hearing, no motor disabilities and normal or corrected-to-normal vision.",
            "startOffset": 31083,
            "title": "Participants"
        },
        {
            "endOffset": 47057,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "Note that non-native listeners show a similar behavioral pattern as native listeners, but that they showed an overall lower accuracy and slower reaction times in the degraded speech conditions (see Fig. 2).",
            "startOffset": 46851,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 63329,
            "parents": [],
            "secId": "s0120",
            "sentence": "Non-native listeners might recruit additional neural resources to process gestural information when speech is clear, by focusing more on gestural information than native listeners.",
            "startOffset": 63149,
            "title": "Conclusion"
        },
        {
            "endOffset": 39041,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0050",
            "sentence": "The EEG was filtered through a 0.02\u2013100 Hz band-pass filter and digitized on-line with a sampling frequency of 500 Hz.",
            "startOffset": 38923,
            "title": "EEG data acquisition"
        },
        {
            "endOffset": 42012,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0070",
            "sentence": "We tested the difference in correct answers and reaction times in two 2 x 2 (Noise-vocoding (clear, degraded) x Gesture (match, mismatch)) repeated measures analysis of variance (ANOVA) per group (native/non-native).",
            "startOffset": 41796,
            "title": "Behavioral results - cued verb-recall task"
        },
        {
            "endOffset": 54241,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "Note that our data revealed stepwise differences between the conditions: the degraded-mismatch condition yielded the largest N400 amplitude, followed by degraded-match, clear-mismatch and clear-match.",
            "startOffset": 54041,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 24338,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0055": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0060": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0105": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0115": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0250": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0280": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0310": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0350": {
                    "endOffset": 24336,
                    "startOffset": 24069
                },
                "b0380": {
                    "endOffset": 24336,
                    "startOffset": 24069
                }
            },
            "secId": "s0015",
            "sentence": "These studies reported differences between native and highly proficient non-native listeners in terms of how previous auditory semantic context is taken into account during adverse listening conditions (Bradlow & Alexander, 2007; Bradlow & Bent, 2002; Gat & Keith, 1978; Golestani, Rosen, & Scott, 2009; Mayo et al., 1997; Oliver, Gullberg, Hellwig, Mitterer, & Indefrey, 2012; Shimizu, Makishima, Yoshida, & Yamagishi, 2002; Wijngaarden et al., 2002; Zhang et al., 2016).",
            "startOffset": 23866,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 52557,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0125": {
                    "endOffset": 52439,
                    "startOffset": 52356
                },
                "b0140": {
                    "endOffset": 52439,
                    "startOffset": 52356
                },
                "b0200": {
                    "endOffset": 52439,
                    "startOffset": 52356
                },
                "b0290": {
                    "endOffset": 52439,
                    "startOffset": 52356
                }
            },
            "secId": "s0110",
            "sentence": "We observed a more negative N400 amplitude when gestures mismatched compared to matched clear speech (in line with e.g., Habets et al., 2011; Holle & Gunter, 2007; Kelly et al., 2004; Ozy\u00fcrek et al., 2007), suggesting that integrating mismatching gestures requires more neural resources than integrating matching gestures.",
            "startOffset": 52235,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 38922,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0050",
            "sentence": "Electrode impedance was kept below 5 KOhm.",
            "startOffset": 38880,
            "title": "EEG data acquisition"
        },
        {
            "endOffset": 55754,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "refoffsets": {
                "b0275": {
                    "endOffset": 55627,
                    "startOffset": 55586
                },
                "b0335": {
                    "endOffset": 55627,
                    "startOffset": 55586
                }
            },
            "secId": "s0110",
            "sentence": "However, post-hoc analyses of the N400 peak latency did not reveal any difference between clear and degraded speech, but only in the onset of the N400 effect (1000 ms for clear speech vs. 1280 ms for degraded speech) Previous studies (e.g., Obleser & Kotz, 2011; Strau\u03b2 et al., 2013) did report significant differences in peak latency in response to degraded speech, suggesting a delayed semantic integration.",
            "startOffset": 55345,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 40976,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "The highest cluster-level statistic from each randomization was entered into the Monte Carlo permutation distribution and cluster-level statistics were calculated for the measured data and compared against this permutation distribution.",
            "startOffset": 40740,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 50349,
            "parents": [],
            "secId": "s0100",
            "sentence": "The current study examined whether and how (non)-native listeners neurally integrate iconic gestures with clear and degraded speech.",
            "startOffset": 50217,
            "title": "Discussion"
        },
        {
            "endOffset": 11805,
            "parents": [],
            "refoffsets": {
                "b0110": {
                    "endOffset": 11803,
                    "startOffset": 11769
                },
                "b0255": {
                    "endOffset": 11803,
                    "startOffset": 11769
                }
            },
            "secId": "s0005",
            "sentence": "Iconic gestures, like that drinking gesture, can be described as hand movements that illustrate object attributes, actions, and space, and can carry semantic information that is relevant to what is conveyed in speech (e.g. Goldin-Meadow, 2005; McNeill, 1992).",
            "startOffset": 11546,
            "title": "Introduction"
        },
        {
            "endOffset": 28313,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "This would then result in higher N400 amplitudes in degraded speech as compared to clear speech.",
            "startOffset": 28217,
            "title": "The present study"
        },
        {
            "endOffset": 35082,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 35014,
                    "startOffset": 34987
                }
            },
            "secId": "s0040",
            "sentence": "Iconicity ratings of the gestures were conducted as part of Drijvers and Ozy\u00fcrek (2017) and revealed a mean recognition rate of 59% when speech was absent.",
            "startOffset": 34927,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 43388,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0075",
            "sentence": "We found a similar pattern in terms of reaction times and found a main effect of Noise-vocoding (F(1, 22) = 74.11, p = <.001, Wilks' Lambda = 0.22, \u03b72 = 0.77), indicating that when the speech signal was clear, native listeners answered more quickly.",
            "startOffset": 43139,
            "title": "Native listeners"
        },
        {
            "endOffset": 15477,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0085": {
                    "endOffset": 15475,
                    "startOffset": 15206
                },
                "b0120": {
                    "endOffset": 15475,
                    "startOffset": 15206
                },
                "b0135": {
                    "endOffset": 15475,
                    "startOffset": 15206
                },
                "b0145": {
                    "endOffset": 15475,
                    "startOffset": 15206
                },
                "b0155": {
                    "endOffset": 15475,
                    "startOffset": 15206
                },
                "b0355": {
                    "endOffset": 15475,
                    "startOffset": 15206
                },
                "b0360": {
                    "endOffset": 15475,
                    "startOffset": 15206
                }
            },
            "secId": "s0010",
            "sentence": "Furthermore, fMRI studies have studied speech-gesture integration from a spatial perspective, and reported involvement of bilateral posterior superior temporal sulcus/middle temporal gyrus (pSTS/MTG) (integration processes) and left inferior frontal gyrus (LIFG) (demanding semantic unification operations, revision/modification) (Dick, Mok, Raja Beharelle, Goldin-Meadow, & Small, 2014; Green et al., 2009; He et al., 2015; Holle, Gunter, Ruschemeyer, Hennenlotter, & Iacoboni, 2008; Holle, Obleser, Rueschemeyer, & Gunter, 2010b; Willems, \u00d6zy\u00fcrek, & Hagoort, 2007; Willems, \u00d6zy\u00fcrek, & Hagoort, 2009).",
            "startOffset": 14875,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 24419,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "However, how these differences are reflected in neural activity remains unknown.",
            "startOffset": 24339,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 33186,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "We presented participants with 160 video clips of a female, native Dutch actress uttering a highly frequent Dutch action verb.",
            "startOffset": 33060,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 50799,
            "parents": [],
            "secId": "s0100",
            "sentence": "Below we will discuss these results in more detail.",
            "startOffset": 50748,
            "title": "Discussion"
        },
        {
            "endOffset": 54040,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "In line with this tentative explanation, we observed an increased N400 amplitude in response to degraded compared to clear speech.",
            "startOffset": 53910,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 57564,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "In the cued-recall task however, the unreliable degraded auditory cues might be more easily recognized when the four answer options were presented.",
            "startOffset": 57417,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 23076,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "The next question is how gestures can enhance clear and degraded speech comprehension in non-native listeners.",
            "startOffset": 22966,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 38879,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0050",
            "sentence": "The ground electrode was placed on the forehead.",
            "startOffset": 38831,
            "title": "EEG data acquisition"
        },
        {
            "endOffset": 56949,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "Compared to native listeners, non-native listeners may have required more neural resources to resolve the degraded auditory cues.",
            "startOffset": 56820,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 40318,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0055",
            "sentence": "Using a multi-level statistical approach, a dependent samples t-test was executed for every data point of two conditions (time by individual by electrode) for the within-group results.",
            "startOffset": 40134,
            "title": "EEG data analysis"
        },
        {
            "endOffset": 48750,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0085",
            "sentence": "To compare the N400 effects in clear and degraded speech, we subtracted the averages of the clear-match from the clear-mismatch condition, and the averages of the degraded-match condition from the degraded-mismatch condition.",
            "startOffset": 48525,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 13736,
            "parents": [],
            "refoffsets": {
                "b0100": {
                    "endOffset": 13734,
                    "startOffset": 13695
                }
            },
            "secId": "s0005",
            "sentence": "They might show more processing difficulties when coupling the semantic information from gesture to the degraded speech signal (see similar mechanisms proposed for difficulty in comprehension of reduced speech in non-natives, Ernestus, Dikmans, and Giezenaar (2017)).",
            "startOffset": 13469,
            "title": "Introduction"
        },
        {
            "endOffset": 37736,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0045",
            "sentence": "Each trial would start with a fixation cross (1000 ms), after which the video started (2000 ms).",
            "startOffset": 37640,
            "title": "Procedure"
        },
        {
            "endOffset": 63892,
            "parents": [],
            "secId": "s0120",
            "sentence": "Thus, although gestures enhance degraded speech comprehension, highly-proficient non-native listeners benefit less from visual semantic context than native listeners and integrate speech and gestures differently.",
            "startOffset": 63680,
            "title": "Conclusion"
        },
        {
            "endOffset": 49392,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0090",
            "sentence": "We observed a significant difference between clear-match and degraded-match (clear-match < degraded-match, p < .001) and between clear-mismatch and degraded-mismatch (clear-mismatch < degraded-mismatch, p < .05) Degraded-mismatch and degraded-match elicited the largest N400 amplitude, followed by clear-mismatch and clear-match.",
            "startOffset": 49063,
            "title": "EEG data - non-native listeners"
        },
        {
            "endOffset": 25920,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "Based on this previous research, one might expect differences in the way speech and gesture are integrated in non-natives and natives in clear and degraded speech contexts.",
            "startOffset": 25748,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 48822,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0085",
            "sentence": "The N400 effect was larger in clear than in degraded speech (p = .041).",
            "startOffset": 48751,
            "title": "EEG data - native participants"
        },
        {
            "endOffset": 51596,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0105",
            "sentence": "Listeners seem to use the semantic information from gestures to boost comprehension when speech is degraded.",
            "startOffset": 51488,
            "title": "Behavioral results - native & non-native listeners"
        },
        {
            "endOffset": 29874,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "On an electrophysiological level, we expected a similar typical N400 effect for highly-proficient non-native listeners during clear speech comprehension when comparing matching and mismatching gestures as in native listeners.",
            "startOffset": 29649,
            "title": "The present study"
        },
        {
            "endOffset": 28001,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0095": {
                    "endOffset": 27696,
                    "startOffset": 27652
                },
                "b0155": {
                    "endOffset": 27696,
                    "startOffset": 27652
                }
            },
            "secId": "s0020",
            "sentence": "Behaviorally, and in line with previous work (Drijvers & Ozy\u00fcrek, 2017; Holle et al., 2010), we expected that native listeners would benefit from gestures during degraded speech comprehension, resulting in more correct answers on the cued-recall task when a gesture matched the speech signal, and faster reaction times for matching than mismatching gestures during degraded speech comprehension.",
            "startOffset": 27606,
            "title": "The present study"
        },
        {
            "endOffset": 33580,
            "parents": [
                {
                    "id": "s0025",
                    "title": "Methods"
                }
            ],
            "secId": "s0040",
            "sentence": "The onset of each video was the same: The actress in the videos would stand in the middle of the screen with her arms hanging casually on each side of her body.",
            "startOffset": 33420,
            "title": "Stimulus materials"
        },
        {
            "endOffset": 29201,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0125": {
                    "endOffset": 29199,
                    "startOffset": 29160
                },
                "b0190": {
                    "endOffset": 29199,
                    "startOffset": 29160
                }
            },
            "secId": "s0020",
            "sentence": "This is in line with speech and gesture comprehension theories that claim that speech and gesture interact to enhance comprehension and that gestures also need speech to be disambiguated (Habets et al., 2011; Kelly et al., 2010).",
            "startOffset": 28972,
            "title": "The present study"
        },
        {
            "endOffset": 24984,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0015",
            "sentence": "Conversely, native listeners could benefit from acoustic and semantic information both in combination and separately.",
            "startOffset": 24867,
            "title": "Non-native speech-gesture processing in clear & adverse listening conditions"
        },
        {
            "endOffset": 16509,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "refoffsets": {
                "b0225": {
                    "endOffset": 16507,
                    "startOffset": 16477
                },
                "b0230": {
                    "endOffset": 16507,
                    "startOffset": 16477
                }
            },
            "secId": "s0010",
            "sentence": "The N400 amplitude is smaller when semantic unification operations are easier (Kutas & Federmeier, 2000, 2014).",
            "startOffset": 16398,
            "title": "Native speech-gesture processing in clear and adverse conditions"
        },
        {
            "endOffset": 30414,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "Non-native listeners might require more neural resources than natives to resolve degraded speech, resulting in a lesser ability to rely on visual semantic information to resolve the phonetic input than native listeners.",
            "startOffset": 30195,
            "title": "The present study"
        },
        {
            "endOffset": 44944,
            "parents": [
                {
                    "id": "s0060",
                    "title": "Results"
                }
            ],
            "secId": "s0080",
            "sentence": "Our analysis revealed a significant main effect of Noise-vocoding, indicating that when speech was clear, non-native listeners had a higher response accuracy than when speech was degraded (F(1, 22) = 165.47, p < .001, Wilks' Lambda = 0.11, \u03b72 = 0.88) and a significant main effect of Gesture, indicating that when a matching gesture was present, non-native listeners were more able to correctly identify the verb than when a mismatching gesture accompanied the verb (F(1, 22) = 69.65, p < .001, Wilks' Lambda = 0.24, \u03b72 = 0.76).",
            "startOffset": 44416,
            "title": "Non-native listeners"
        },
        {
            "endOffset": 53513,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "Although we did not provide listeners with prior auditory context similar to the studies mentioned above, we did provide listeners with a visual semantic context.",
            "startOffset": 53351,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 54677,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "We also observed an N400 effect in both degraded speech and clear speech, which shows that gestures exert a visual semantic context effect.",
            "startOffset": 54538,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 61525,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0115",
            "sentence": "In clear speech, non-native listeners might attempt to exploit and process the semantic information from gestural input more than native listeners, resulting in the recruitment of right-lateralized areas in the heightened processing of the semantic information that is provided by the gesture.",
            "startOffset": 61232,
            "title": "EEG results - Non-native listeners"
        },
        {
            "endOffset": 54856,
            "parents": [
                {
                    "id": "s0100",
                    "title": "Discussion"
                }
            ],
            "secId": "s0110",
            "sentence": "This N400 effect was reduced in degraded speech, which is possibly due to the fact that listeners have less auditory cues to their disposal to couple the gestural information to.",
            "startOffset": 54678,
            "title": "EEG results - native listeners"
        },
        {
            "endOffset": 26575,
            "parents": [
                {
                    "id": "s0005",
                    "title": "Introduction"
                }
            ],
            "secId": "s0020",
            "sentence": "We present an EEG study that aims to further our understanding of how native and non-native listeners integrate information online from speech and iconic co-speech gestures during both clear and degraded speech comprehension.",
            "startOffset": 26350,
            "title": "The present study"
        }
    ],
    "docId": "S0093934X1730041X",
    "metadata": {
        "asjc": [
            "1203",
            "2805",
            "3205",
            "3310",
            "3616"
        ],
        "authors": [
            {
                "email": "linda.drijvers@mpi.nl",
                "first": "Linda",
                "initial": "L.",
                "last": "Drijvers"
            },
            {
                "email": null,
                "first": "Asli",
                "initial": "A.",
                "last": "\u00d6zy\u00fcrek"
            }
        ],
        "doi": "10.1016/j.bandl.2018.01.003",
        "firstpage": "7",
        "issn": "0093934X",
        "lastpage": "17",
        "openaccess": "Full",
        "pub_year": 2018,
        "subjareas": [
            "ARTS",
            "HEAL",
            "NEUR",
            "PSYC",
            "SOCI"
        ],
        "title": "Native language status of the listener modulates the neural integration of speech and iconic gestures in clear and adverse listening conditions"
    }
}